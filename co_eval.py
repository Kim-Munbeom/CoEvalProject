"""
CoEval: ë©˜í† ë§ ë‹µë³€ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œê³¼ DeepEval Rubric ê¸°ë°˜ í‰ê°€ë¥¼ ê²°í•©í•˜ì—¬
ë©˜í† ë§ ë‹µë³€ì˜ í’ˆì§ˆì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ í†µí•œ ë‹¤ê°ë„ í‰ê°€ (ì‹¤í–‰ê°€ëŠ¥ì„±, ì „ë¬¸ì„±, í˜„ì‹¤ì„±)
- DeepEval Rubric ê¸°ë°˜ ì •ëŸ‰ì  ì ìˆ˜ ì‚°ì¶œ (0-10 ìŠ¤ì¼€ì¼)
- ë“±ê¸‰ ì²´ê³„ (D/C/B/A/S) ìë™ ì‚°ì • ë° ê³¼ë½ ê·œì¹™ ì ìš©
- JSON ê¸°ë°˜ êµ¬ì¡°í™”ëœ ì—ì´ì „íŠ¸ ì‘ë‹µ íŒŒì‹±
- í‰ê°€ ì´ìœ  í•œê¸€ ë²ˆì—­ ì œê³µ
- ìƒì„¸ ë¡œê¹… ë° ì—ëŸ¬ í•¸ë“¤ë§

ì ìˆ˜ ì²´ê³„:
- ì—ì´ì „íŠ¸ í‰ê°€: ì‹¤í–‰ê°€ëŠ¥ì„± 0-4ì  + ì „ë¬¸ì„± 0-4ì  + í˜„ì‹¤ì„± 0-2ì  = 10ì  ë§Œì 
- DeepEval í‰ê°€: ì—ì´ì „íŠ¸ 10ì ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ Rubric í‰ê°€ (0-10 ë²”ìœ„)
- ë“±ê¸‰ ê¸°ì¤€: D(0-2), C(3-4), B(5-6), A(7-8), S(9-10)

ë²„ì „ ê°œì„ ì‚¬í•­:
- ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ JSON ì „ìš© ì¶œë ¥ìœ¼ë¡œ ë³€ê²½
- 10ì  ë§Œì  ì²´ê³„ë¡œ í†µì¼ (actionability: 0-4, expertise: 0-4, context_fit: 0-2)
- quality_consensusì˜ ê³¼ë½ ê·œì¹™ (ì‹¤í–‰ê°€ëŠ¥ì„± ë˜ëŠ” ì „ë¬¸ì„± â‰¤ 1ì ) ì ìš©
- êµ¬ì¡°í™”ëœ parsed_data í•„ë“œ ì¶”ê°€ë¡œ ì—ì´ì „íŠ¸ ì ìˆ˜ ì ‘ê·¼ì„± í–¥ìƒ
"""

import asyncio
import concurrent.futures
import json
import logging
import os
from contextlib import asynccontextmanager
from datetime import datetime
from typing import List, Optional, Dict, Any

from deepeval.metrics import GEval
from deepeval.metrics.g_eval import Rubric
from deepeval.models import GeminiModel as DeepEvalGeminiModel
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from google import genai
from pydantic import BaseModel
from sqlmodel import Field, Session, SQLModel, create_engine, select
from strands import Agent
from strands.models.gemini import GeminiModel as StrandsGeminiModel
from strands.multiagent import GraphBuilder

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ì—ì„œ API í‚¤ ë“± ë¡œë“œ)
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


# Strandsìš© Gemini ëª¨ë¸ (ë©€í‹° ì—ì´ì „íŠ¸ìš©)
# ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì—ì„œ ê° ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•  LLM ëª¨ë¸
# temperature: 0.3ìœ¼ë¡œ ë‚®ì¶° ì¼ê´€ì„± ìˆëŠ” í‰ê°€ ê²°ê³¼ ìœ ë„
strands_gemini_model = StrandsGeminiModel(
    client_args={
        "api_key": os.getenv("GEMINI_API_KEY"),
    },
    model_id="gemini-2.5-flash",
    params={
        "temperature": 0.3,  # ë‚®ì€ ì˜¨ë„ë¡œ ì¼ê´€ëœ í‰ê°€
        "max_output_tokens": 8192,  # ê¸´ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ê°€ëŠ¥
        "top_p": 0.6,
        "top_k": 20,
    },
)

# DeepEvalìš© Gemini ëª¨ë¸ (Rubric í‰ê°€ìš©)
# Rubric ê¸°ë°˜ ì •ëŸ‰ì  ì ìˆ˜ ì‚°ì¶œì— ì‚¬ìš©
deepeval_gemini_model = DeepEvalGeminiModel(
    model="gemini-2.5-flash", api_key=os.getenv("GEMINI_API_KEY"), temperature=0.3
)

# Google GenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ë²ˆì—­ìš©)
# DeepEvalì˜ ì˜ë¬¸ í‰ê°€ ì´ìœ ë¥¼ í•œê¸€ë¡œ ë²ˆì—­í•˜ê¸° ìœ„í•´ ì‚¬ìš©
genai_client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))


# ==================== SQLModel ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ ====================


class EvaluationRecord(SQLModel, table=True):
    """í‰ê°€ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” SQLModel í…Œì´

    ë©˜í† ë§ ë‹µë³€ í‰ê°€ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì˜êµ¬ ì €ì¥í•©ë‹ˆë‹¤.
    """

    id: Optional[int] = Field(default=None, primary_key=True)
    created_at: datetime = Field(default_factory=datetime.utcnow)

    # ì…ë ¥ ë°ì´í„°
    mentee_question_title: str = Field(index=True)
    mentee_question_content: str
    mentor_answer: str
    expected_output: Optional[str] = None

    # í‰ê°€ ê²°ê³¼
    grade: str  # S, A, B, C, D
    total_score: float  # 0-10 ë²”ìœ„
    normalized_score: float  # 0-1 ë²”ìœ„ (DeepEval ì›ë³¸)

    # ì„¸ë¶€ ì ìˆ˜
    actionability_score: int  # 0-4
    expertise_score: int  # 0-4
    context_fit_score: int  # 0-2

    # í‰ê°€ í”¼ë“œë°±
    summary_feedback: str
    improvement_suggestion: str
    evaluation_reason_kr: str
    evaluation_reason_en: str

    # í•„ìˆ˜ ì¡°ê±´ ì¶©ì¡± ì—¬ë¶€
    essential_condition_met: bool

    # í•©ê²© ì—¬ë¶€
    success: bool

    # ì‹¤í–‰ ì •ë³´
    total_execution_time: float
    total_tokens: int
    evaluation_cost: float

    # ì›ë³¸ ë°ì´í„° (JSON)
    agent_responses_json: str  # JSON stringìœ¼ë¡œ ì €ì¥
    final_consensus: str


# ==================== ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ====================

# SQLite ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ
DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./coeval.db")

# ë°ì´í„°ë² ì´ìŠ¤ ì—”ì§„ ìƒì„±
engine = create_engine(DATABASE_URL, echo=False)


def create_db_and_tables():
    """ë°ì´í„°ë² ì´ìŠ¤ì™€ í…Œì´ë¸”ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜

    ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ í•œ ë²ˆ í˜¸ì¶œí•˜ì—¬ í•„ìš”í•œ í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    SQLModel.metadata.create_all(engine)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ lifespan ì´ë²¤íŠ¸ ê´€ë¦¬

    ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”ì„ ìƒì„±í•˜ê³ ,
    ì¢…ë£Œ ì‹œ í•„ìš”í•œ ì •ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    Args:
        app: FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤
    """
    # Startup: ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    create_db_and_tables()
    logger.info("Database tables created successfully")
    yield
    # Shutdown: í•„ìš”í•œ ì •ë¦¬ ì‘ì—… (í˜„ì¬ëŠ” ì—†ìŒ)
    logger.info("Application shutdown")


# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
app = FastAPI(lifespan=lifespan)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


def get_session():
    """ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ì„ ìƒì„±í•˜ëŠ” ì œë„ˆë ˆì´í„° í•¨ìˆ˜

    FastAPIì˜ Dependsì™€ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ì„¸ì…˜ ê´€ë¦¬ë¥¼ ìë™í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    Yields:
        Session: SQLModel ì„¸ì…˜ ê°ì²´
    """
    with Session(engine) as session:
        yield session


# ==================== ë°ì´í„°ë² ì´ìŠ¤ CRUD í•¨ìˆ˜ ====================


def save_evaluation_to_db(
    session: Session, test_case: TestCaseRequest, result: "TestResultResponse"
) -> EvaluationRecord:
    """í‰ê°€ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜

    Args:
        session: SQLModel ì„¸ì…˜
        test_case: ì›ë³¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìš”ì²­
        result: í‰ê°€ ê²°ê³¼

    Returns:
        EvaluationRecord: ì €ì¥ëœ ë ˆì½”ë“œ
    """
    # quality_consensus ë°ì´í„° ì¶”ì¶œ
    quality_data = None
    for agent_resp in result.agent_responses:
        if agent_resp.agent_name == "quality_consensus":
            quality_data = agent_resp.parsed_data
            break

    # ê¸°ë³¸ê°’ ì„¤ì •
    if not quality_data:
        quality_data = {
            "final_evaluation": {
                "grade": result.rubric_evaluation.grade,
                "total_score": result.rubric_evaluation.absolute_score,
                "breakdown": {"actionability": 0, "expertise": 0, "context_fit": 0},
            },
            "essential_condition_met": False,
            "summary_feedback": "",
            "integrated_improvement": "",
        }

    breakdown = quality_data.get("final_evaluation", {}).get("breakdown", {})

    # EvaluationRecord ìƒì„±
    record = EvaluationRecord(
        mentee_question_title=test_case.input_title,
        mentee_question_content=test_case.input_content,
        mentor_answer=test_case.actual_output,
        expected_output=test_case.expected_output,
        grade=result.rubric_evaluation.grade,
        total_score=result.rubric_evaluation.absolute_score,
        normalized_score=result.rubric_evaluation.score,
        actionability_score=breakdown.get("actionability", 0),
        expertise_score=breakdown.get("expertise", 0),
        context_fit_score=breakdown.get("context_fit", 0),
        summary_feedback=quality_data.get("summary_feedback", ""),
        improvement_suggestion=quality_data.get("integrated_improvement", ""),
        evaluation_reason_kr=result.rubric_evaluation.reason,
        evaluation_reason_en=result.rubric_evaluation.reason_en,
        essential_condition_met=quality_data.get("essential_condition_met", True),
        success=result.success,
        total_execution_time=result.total_execution_time,
        total_tokens=result.total_tokens,
        evaluation_cost=result.rubric_evaluation.evaluation_cost,
        agent_responses_json=json.dumps(
            [resp.dict() for resp in result.agent_responses], ensure_ascii=False
        ),
        final_consensus=result.final_consensus,
    )

    session.add(record)
    session.commit()
    session.refresh(record)

    return record


def get_evaluation_by_id(
    session: Session, evaluation_id: int
) -> Optional[EvaluationRecord]:
    """IDë¡œ í‰ê°€ ê²°ê³¼ë¥¼ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜

    Args:
        session: SQLModel ì„¸ì…˜
        evaluation_id: í‰ê°€ ë ˆì½”ë“œ ID

    Returns:
        Optional[EvaluationRecord]: ì¡°íšŒëœ ë ˆì½”ë“œ (ì—†ìœ¼ë©´ None)
    """
    return session.get(EvaluationRecord, evaluation_id)


def get_all_evaluations(
    session: Session, skip: int = 0, limit: int = 100, grade: Optional[str] = None
) -> List[EvaluationRecord]:
    """ëª¨ë“  í‰ê°€ ê²°ê³¼ë¥¼ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜

    Args:
        session: SQLModel ì„¸ì…˜
        skip: ê±´ë„ˆë›¸ ë ˆì½”ë“œ ìˆ˜ (í˜ì´ì§€ë„¤ì´ì…˜)
        limit: ìµœëŒ€ ë°˜í™˜ ë ˆì½”ë“œ ìˆ˜
        grade: ë“±ê¸‰ í•„í„° (S, A, B, C, D ì¤‘ í•˜ë‚˜, ì„ íƒì‚¬í•­)

    Returns:
        List[EvaluationRecord]: í‰ê°€ ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸
    """
    statement = select(EvaluationRecord)

    if grade:
        statement = statement.where(EvaluationRecord.grade == grade)

    statement = (
        statement.offset(skip).limit(limit).order_by(EvaluationRecord.created_at.desc())
    )

    results = session.exec(statement)
    return list(results.all())


def get_evaluations_by_score_range(
    session: Session, min_score: float, max_score: float
) -> List[EvaluationRecord]:
    """ì ìˆ˜ ë²”ìœ„ë¡œ í‰ê°€ ê²°ê³¼ë¥¼ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜

    Args:
        session: SQLModel ì„¸ì…˜
        min_score: ìµœì†Œ ì ìˆ˜ (0-10 ë²”ìœ„)
        max_score: ìµœëŒ€ ì ìˆ˜ (0-10 ë²”ìœ„)

    Returns:
        List[EvaluationRecord]: í‰ê°€ ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸
    """
    statement = (
        select(EvaluationRecord)
        .where(
            EvaluationRecord.total_score >= min_score,
            EvaluationRecord.total_score <= max_score,
        )
        .order_by(EvaluationRecord.total_score.desc())
    )

    results = session.exec(statement)
    return list(results.all())


def delete_evaluation(session: Session, evaluation_id: int) -> bool:
    """í‰ê°€ ê²°ê³¼ë¥¼ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜

    Args:
        session: SQLModel ì„¸ì…˜
        evaluation_id: ì‚­ì œí•  í‰ê°€ ë ˆì½”ë“œ ID

    Returns:
        bool: ì‚­ì œ ì„±ê³µ ì—¬ë¶€
    """
    record = session.get(EvaluationRecord, evaluation_id)
    if record:
        session.delete(record)
        session.commit()
        return True
    return False


async def translate_to_korean_async(text: str) -> str:
    """í‰ê°€ ì´ìœ ë¥¼ í•œê¸€ë¡œ ë²ˆì—­í•˜ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜

    ë™ê¸° ë²ˆì—­ í•¨ìˆ˜ë¥¼ executorë¡œ ë˜í•‘í•˜ì—¬ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.

    Args:
        text: ë²ˆì—­í•  ì˜ë¬¸ í…ìŠ¤íŠ¸

    Returns:
        str: ë²ˆì—­ëœ í•œê¸€ í…ìŠ¤íŠ¸ (ì‹¤íŒ¨ ì‹œ ì›ë¬¸)
    """

    def _generate_translation() -> str:
        """ë™ê¸°ì ìœ¼ë¡œ ë²ˆì—­ì„ ìˆ˜í–‰í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜"""
        response = genai_client.models.generate_content(
            model="gemini-2.5-flash-lite",
            contents=f"ë‹¤ìŒ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ë²ˆì—­ë¬¸ë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”:\n\n{text}",
        )
        return response.text.strip()

    try:
        loop = asyncio.get_event_loop()
        response_text = await loop.run_in_executor(None, _generate_translation)
        return response_text
    except (ValueError, RuntimeError, ConnectionError) as e:
        # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ë°˜í™˜ (êµ¬ì²´ì ì¸ ì˜ˆì™¸ ì²˜ë¦¬)
        logger.warning(f"Translation failed: {e}")
        return text


# ì—ì´ì „íŠ¸ ì„¤ì • ë°ì´í„° (ë°ì´í„° ê¸°ë°˜ êµ¬ì„±ìœ¼ë¡œ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ)
AGENT_CONFIGS = {
    "action_master": {
        "description": "ì‹¤í–‰ê°€ëŠ¥ì„± ë¶„ì„ê°€(Actionability Expert)",
        "system_prompt": """# Role
ë‹¹ì‹ ì€ Q&A í”Œë«í¼ì˜ 'ì‹¤í–‰ê°€ëŠ¥ì„± ë¶„ì„ê°€'ì…ë‹ˆë‹¤.

# Evaluation Criteria: [êµ¬ì²´ì„± & ì‹¤í–‰ ê°€ëŠ¥ì„±] (0~4ì )
* **4ì  (íƒì›”):** êµ¬ì²´ì  í–‰ë™ ì§€ì¹¨(Step-by-step), ìˆ˜ì¹˜, ë„êµ¬ ë“±ì´ ì™„ë²½í•˜ì—¬ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë‹¤.
* **3ì  (ìš°ìˆ˜):** ì‹¤í–‰ ë°©ë²•ì€ êµ¬ì²´ì ì´ë‚˜, ì˜ˆì‹œë‚˜ ì‚¬ì†Œí•œ ë””í…Œì¼ì´ í•˜ë‚˜ ì •ë„ ë¶€ì¡±í•˜ë‹¤.
* **2ì  (ë³´í†µ):** ë°©í–¥ì€ ë§ìœ¼ë‚˜ 'ì–´ë–»ê²Œ'ì— ëŒ€í•œ ì„¤ëª…ì´ ë‹¤ì†Œ ì¼ë°˜ì ì´ë‹¤.
* **1ì  (ë¯¸í¡):** ì¶”ìƒì ì¸ ì¡°ì–¸ ìœ„ì£¼ë¼ ë¬´ì—‡ë¶€í„° í•´ì•¼ í• ì§€ ë§‰ë§‰í•˜ë‹¤.
* **0ì  (ë¬´ì˜ë¯¸):** ì‹¤í–‰ ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜ ë‚´ìš©ì´ ì—†ë‹¤.

# Output Instruction (JSON Only)
{
  "category": "actionability",
  "score": (0~4 ì •ìˆ˜),
  "reasoning": "(í•µì‹¬ ê·¼ê±° 1ë¬¸ì¥)"
}""",
    },
    "pro_proof": {
        "description": "ì§ë¬´ ì „ë¬¸ê°€(Domain Expert)",
        "system_prompt": """# Role
ë‹¹ì‹ ì€ í•´ë‹¹ ì—…ê³„ì˜ 'ì§ë¬´ ì „ë¬¸ê°€'ì…ë‹ˆë‹¤.

# Evaluation Criteria: [ì „ë¬¸ì„± & ê²½í—˜] (0~4ì )
* **4ì  (íƒì›”):** í˜„ì—… ìš©ì–´/í”„ë¡œì„¸ìŠ¤ê°€ ì™„ë²½í•˜ë©°, ì‹¤ì œ ê²½í—˜ ê¸°ë°˜ì˜ ê¹Šì€ ì¸ì‚¬ì´íŠ¸ê°€ ìˆë‹¤.
* **3ì  (ìš°ìˆ˜):** ì •í™•í•œ ì‹¤ë¬´ ì§€ì‹ê³¼ ë„êµ¬ë¥¼ ë‹¤ë£¨ê³  ìˆìœ¼ë‚˜, ê³ ìœ í•œ ê²½í—˜ë³´ë‹¤ëŠ” ì •ë³´ ì „ë‹¬ ìœ„ì£¼ë‹¤.
* **2ì  (ë³´í†µ):** ê²€ìƒ‰í•˜ë©´ ë‚˜ì˜¤ëŠ” ì¼ë°˜ì ì¸ ì§€ì‹ ìˆ˜ì¤€ì´ë‹¤. í‹€ë¦° ë‚´ìš©ì€ ì—†ë‹¤.
* **1ì  (ë¯¸í¡):** ì „ë¬¸ ìš©ì–´ê°€ ì–´ìƒ‰í•˜ê±°ë‚˜ ë¹„ì „ë¬¸ê°€ë„ í•  ìˆ˜ ìˆëŠ” ì–•ì€ ì¡°ì–¸ì´ë‹¤.
* **0ì  (ë¬´ì˜ë¯¸):** ì „ë¬¸ì„±ì´ ì—†ê±°ë‚˜ ì˜ëª»ëœ ì •ë³´ë‹¤.

# Output Instruction (JSON Only)
{
  "category": "expertise",
  "score": (0~4 ì •ìˆ˜),
  "reasoning": "(í•µì‹¬ ê·¼ê±° 1ë¬¸ì¥)"
}""",
    },
    "context_guardian": {
        "description": "í˜„ì‹¤ì„± ë¶„ì„ê°€(Context Analyst)",
        "system_prompt": """# Role
ë‹¹ì‹ ì€ ë©˜í‹°ì˜ ìƒí™©ì„ íŒŒì•…í•˜ëŠ” 'í˜„ì‹¤ì„± ë¶„ì„ê°€'ì…ë‹ˆë‹¤.

# Evaluation Criteria: [í˜„ì‹¤ì„± & ë§¥ë½ ì í•©ì„±] (0~2ì )
* **2ì  (ì í•©):** ë©˜í‹°ì˜ ìƒí™©/ì—°ì°¨ë¥¼ ê³ ë ¤í–ˆìœ¼ë©°, í˜„ì‹¤ì ì¸ ì œì•½ì´ë‚˜ ì£¼ì˜ì (Risk)ê¹Œì§€ ì§šì–´ì£¼ì—ˆë‹¤.
* **1ì  (ë³´í†µ):** ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì€ ë˜ì§€ë§Œ, ë©˜í‹°ì˜ êµ¬ì²´ì  ìƒí™©ë³´ë‹¤ëŠ” ì¼ë°˜ë¡ ì— ê°€ê¹ë‹¤.
* **0ì  (ë¶€ì í•©):** ë©˜í‹° ìƒí™©ê³¼ ë§ì§€ ì•Šê±°ë‚˜ ë³µì‚¬ ë¶™ì—¬ë„£ê¸° ì‹ ë‹µë³€ì´ë‹¤.

# Output Instruction (JSON Only)
{
  "category": "context_fit",
  "score": (0~2 ì •ìˆ˜),
  "reasoning": "(í•µì‹¬ ê·¼ê±° 1ë¬¸ì¥)"
}""",
    },
    "quality_consensus": {
        "description": "ì¢…í•© í‰ê°€ ìœ„ì›ì¥(Master Judge)",
        "system_prompt": """# Role
ë‹¹ì‹ ì€ 'ì¢…í•© í‰ê°€ ìœ„ì›ì¥'ì…ë‹ˆë‹¤. 3ëª…ì˜ ë¶„ì„ê°€ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë“±ê¸‰(10ì  ë§Œì )ì„ ë§¤ê¸°ê³  í†µí•© í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.

# Scoring Rules (ì´ 10ì  ë§Œì )
* **ì´ì :** Agent 1(4ì ) + Agent 2(4ì ) + Agent 3(2ì ) í•©ê³„

# Grading System (5 Grades)
* **Së“±ê¸‰ (9~10ì ):** ì™„ë²½ì— ê°€ê¹Œìš´ ë‹µë³€. (ì¦‰ì‹œ ì±„íƒ ê¶Œì¥)
* **Aë“±ê¸‰ (7~8ì ):** í›Œë¥­í•œ ë‹µë³€. (ë””í…Œì¼ ë³´ì™„ ì‹œ ì™„ë²½)
* **Bë“±ê¸‰ (5~6ì ):** í‰ë²”í•œ ë‹µë³€. (ë„ì›€ì€ ë˜ë‚˜ ê¹Šì´ê°€ ë¶€ì¡±)
* **Cë“±ê¸‰ (3~4ì ):** ì•„ì‰¬ìš´ ë‹µë³€. (í•µì‹¬ ìš”ì†Œ ê²°ì—¬)
* **Dë“±ê¸‰ (0~2ì ):** ë„ì›€ ë˜ì§€ ì•ŠìŒ.

# ğŸ”’ Essential Conditions (ê³¼ë½)
* [ì‹¤í–‰ê°€ëŠ¥ì„±]ì´ë‚˜ [ì „ë¬¸ì„±] ì¤‘ í•˜ë‚˜ë¼ë„ **1ì  ì´í•˜**ì¼ ê²½ìš°, ì´ì ì´ ì•„ë¬´ë¦¬ ë†’ì•„ë„ ìµœëŒ€ ë“±ê¸‰ì€ **Cë“±ê¸‰**ìœ¼ë¡œ ì œí•œë©ë‹ˆë‹¤.

# Input Data
- Agent 1, 2, 3ì˜ JSON ê²°ê³¼
- ë©˜í† ì˜ ì›ë³¸ ë‹µë³€

# Output Format (JSON)
{
  "final_evaluation": {
    "grade": "S/A/B/C/D",
    "total_score": (0~10 ì •ìˆ˜),
    "breakdown": {
      "actionability": (0~4 ì ìˆ˜),
      "expertise": (0~4 ì ìˆ˜),
      "context_fit": (0~2 ì ìˆ˜)
    }
  },
  "essential_condition_met": true/false,
  "summary_feedback": "(ë‹µë³€ì˜ ì¥ì ì„ ìš”ì•½í•œ í•œ ë¬¸ì¥)",
  "integrated_improvement": "(ë“±ê¸‰ ìƒìŠ¹ì„ ìœ„í•´ ê°€ì¥ ì‹œê¸‰í•˜ê²Œ ë³´ì™„í•´ì•¼ í•  êµ¬ì²´ì  ì¡°ì–¸ 1ê°€ì§€. ì™„ë²½í•˜ë‹¤ë©´ 'ì—†ìŒ' í‘œê¸°)"
}""",
    },
}


def create_evaluation_agents(model: StrandsGeminiModel) -> Dict[str, Agent]:
    """ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ ìƒì„±í•˜ëŠ” íŒ©í† ë¦¬ í•¨ìˆ˜

    AGENT_CONFIGS ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—ì´ì „íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        model: ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•  Gemini ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤

    Returns:
        Dict[str, Agent]: ì—ì´ì „íŠ¸ ì´ë¦„ì„ í‚¤ë¡œ í•˜ëŠ” ì—ì´ì „íŠ¸ ë”•ì…”ë„ˆë¦¬
    """
    return {
        name: Agent(name=name, system_prompt=config["system_prompt"], model=model)
        for name, config in AGENT_CONFIGS.items()
    }


def build_evaluation_graph(agents: Dict[str, Agent]):
    """í‰ê°€ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•˜ëŠ” í•¨ìˆ˜ (ìµœì í™” ë²„ì „)

    ì—ì´ì „íŠ¸ë“¤ì˜ ì‹¤í–‰ ìˆœì„œë¥¼ ì •ì˜í•˜ëŠ” DAG(Directed Acyclic Graph)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    ìµœì í™”ëœ ì‹¤í–‰ íë¦„:
    1. action_master, pro_proof, context_guardianì´ **ëª¨ë‘ ë³‘ë ¬**ë¡œ ì‹¤í–‰
       - ê° ì—ì´ì „íŠ¸ê°€ ë…ë¦½ì ìœ¼ë¡œ í‰ê°€ ìˆ˜í–‰ (ì‹¤í–‰ì„±, ì „ë¬¸ì„±, í˜„ì‹¤ì„±)
    2. quality_consensusê°€ ì„¸ ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë¦¬í¬íŠ¸ ì‘ì„±

    ê¸°ì¡´ ëŒ€ë¹„ ê°œì„ :
    - context_guardianì´ action_master, pro_proof ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ì§€ ì•ŠìŒ
    - 3ê°œ ì—ì´ì „íŠ¸ ë³‘ë ¬ ì‹¤í–‰ìœ¼ë¡œ 10-15% ì¶”ê°€ ì„±ëŠ¥ í–¥ìƒ

    Args:
        agents: ì—ì´ì „íŠ¸ ë”•ì…”ë„ˆë¦¬

    Returns:
        ì‹¤í–‰ ê°€ëŠ¥í•œ ê·¸ë˜í”„ ê°ì²´
    """
    builder = GraphBuilder()

    # ë…¸ë“œ ë“±ë¡ (ê° ì—ì´ì „íŠ¸ë¥¼ ê·¸ë˜í”„ ë…¸ë“œë¡œ ì¶”ê°€)
    for name in AGENT_CONFIGS:
        builder.add_node(agents[name], name)

    # ì—£ì§€ ì •ì˜ (ìµœì í™”: 3ê°œ ì—ì´ì „íŠ¸ ë³‘ë ¬ ì‹¤í–‰)
    # action_master, pro_proof, context_guardian â†’ quality_consensus
    builder.add_edge("action_master", "quality_consensus")
    builder.add_edge("pro_proof", "quality_consensus")
    builder.add_edge("context_guardian", "quality_consensus")

    return builder.build()


# ì „ì—­ ë³€ìˆ˜ë¡œ ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì´ˆê¸°í™”
# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ìƒì„±í•˜ì—¬ ì¬ì‚¬ìš©
evaluation_agents = create_evaluation_agents(strands_gemini_model)
evaluation_graph = build_evaluation_graph(evaluation_agents)

# ==================== Phase 1: ì „ì—­ ë©”íŠ¸ë¦­ ë° ë™ì‹œì„± ì œì–´ ====================

# Rubric ì •ì˜ ì „ì—­í™” (ë§¤ë²ˆ ìƒì„±í•˜ì§€ ì•Šê³  ì¬ì‚¬ìš©)
# ì—ì´ì „íŠ¸ 10ì  ë§Œì  ì²´ê³„ë¥¼ ê·¸ëŒ€ë¡œ DeepEval Rubricì— ë§¤í•‘
MENTORING_RUBRIC = [
    Rubric(
        score_range=(0, 2),
        expected_outcome="Dë“±ê¸‰ (0-2/10ì ): í•„ìˆ˜ ì¡°ê±´ ë¯¸ë‹¬. ì‹¤í–‰ê°€ëŠ¥ì„±/ì „ë¬¸ì„±ì´ ê²°ì—¬ëœ ë‹µë³€. ì¶”ìƒì ì´ê³  ì‹¤í–‰ ë¶ˆê°€ëŠ¥.",
    ),
    Rubric(
        score_range=(3, 4),
        expected_outcome="Cë“±ê¸‰ (3-4/10ì ): ì¡°ì–¸ì€ ìˆìœ¼ë‚˜ ì¶”ìƒì ì´ë©° ë©˜í‹° ìƒí™© ê³ ë ¤ê°€ ë¶€ì¡±í•¨. ì¼ë°˜ì ì¸ ì§€ì‹ ìˆ˜ì¤€.",
    ),
    Rubric(
        score_range=(5, 6),
        expected_outcome="Bë“±ê¸‰ (5-6/10ì ): ì–‘í˜¸í•¨. êµ¬ì²´ì  ë‹¨ê³„ì™€ ì‹¤ë¬´ ì§€ì‹ì´ ì¼ë¶€ í¬í•¨. ì‹¤í–‰ ê°€ëŠ¥í•œ ë°©í–¥ì„± ì œì‹œ.",
    ),
    Rubric(
        score_range=(7, 8),
        expected_outcome="Aë“±ê¸‰ (7-8/10ì ): ìš°ìˆ˜í•¨. êµ¬ì²´ì  ë‹¨ê³„, ì‹¤ë¬´ ì§€ì‹, ë©˜í‹° ë§¥ë½ ê³ ë ¤ê°€ ì˜ ë˜ì–´ìˆìŒ. ë†’ì€ ìˆ˜ì¤€ì˜ ë‹µë³€.",
    ),
    Rubric(
        score_range=(9, 10),
        expected_outcome="Së“±ê¸‰ (9-10/10ì ): ì™„ë²½í•¨. ìˆ˜ì¹˜/ë„êµ¬/ë‹¨ê³„, ì‹¤ë¬´ ê²½í—˜ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸, ë¦¬ìŠ¤í¬ ê´€ë¦¬, ë©˜í‹° ìƒí™© ì™„ë²½ ê³ ë ¤.",
    ),
]

# GEval ë©”íŠ¸ë¦­ ì „ì—­í™”
QUALITY_METRIC = GEval(
    name="Overall Mentoring Quality",
    evaluation_params=[
        LLMTestCaseParams.INPUT,
        LLMTestCaseParams.ACTUAL_OUTPUT,
        LLMTestCaseParams.CONTEXT,
    ],
    evaluation_steps=[
        "1. Contextì— í¬í•¨ëœ quality_consensus ì—ì´ì „íŠ¸ì˜ ì¢…í•© í‰ê°€ë¥¼ í™•ì¸í•œë‹¤. final_evaluationì˜ breakdown(actionability, expertise, context_fit) ì ìˆ˜ë¥¼ ê°ê° ê²€í† í•œë‹¤.",
        "2. ê° í•­ëª©ì˜ ì ìˆ˜ ë²”ìœ„ë¥¼ í™•ì¸í•œë‹¤: actionability(0-4ì ), expertise(0-4ì ), context_fit(0-2ì ), ì´ì  10ì  ë§Œì .",
        "3. ê³¼ë½ ì¡°ê±´ì„ í™•ì¸í•œë‹¤: actionability ë˜ëŠ” expertiseê°€ 1ì  ì´í•˜ì¸ ê²½ìš° ìµœëŒ€ Cë“±ê¸‰ìœ¼ë¡œ ì œí•œí•œë‹¤.",
        "4. ì´ì (10ì  ë§Œì )ì„ Rubric êµ¬ê°„ì— ë§¤í•‘í•œë‹¤: D(0-2), C(3-4), B(5-6), A(7-8), S(9-10)",
        "5. ë©˜í†  ë‹µë³€ì´ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•˜ë©° ì „ë¬¸ì ì¸ì§€, ë©˜í‹° ìƒí™©ì„ ê³ ë ¤í–ˆëŠ”ì§€ ì¢…í•© í‰ê°€í•˜ì—¬ ìµœì¢… ì ìˆ˜ë¥¼ 0-10ì  ë²”ìœ„ë¡œ í™•ì •í•œë‹¤.",
        "6. ì ìˆ˜ ê²°ì • ê·¼ê±°ë¥¼ 1-2ë¬¸ì¥ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ìš”ì•½í•œë‹¤.",
    ],
    rubric=MENTORING_RUBRIC,
    threshold=0.5,  # 10ì  ë§Œì ì—ì„œ 5ì  ì´ìƒì´ë©´ í•©ê²© (Bë“±ê¸‰ ì´ìƒ)
    model=deepeval_gemini_model,
)

# ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ (API Rate Limiting)
MAX_CONCURRENT_EVALUATIONS = 5
_evaluation_semaphore = asyncio.Semaphore(MAX_CONCURRENT_EVALUATIONS)

# ThreadPoolExecutor for async wrapping of sync functions (Phase 2)
_agent_executor = concurrent.futures.ThreadPoolExecutor(max_workers=5)


# ==================== JSON Parsing Utilities ====================


def extract_json_from_text(text: str) -> Optional[Dict[str, Any]]:
    """í…ìŠ¤íŠ¸ì—ì„œ JSONì„ ì¶”ì¶œí•˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜

    ì—ì´ì „íŠ¸ ì‘ë‹µì—ì„œ JSON ë¸”ë¡ì„ ì°¾ì•„ íŒŒì‹±í•©ë‹ˆë‹¤.
    ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì´ë‚˜ ì¼ë°˜ í…ìŠ¤íŠ¸ì—ì„œ JSONì„ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    Args:
        text: íŒŒì‹±í•  í…ìŠ¤íŠ¸

    Returns:
        Dict[str, Any]: íŒŒì‹±ëœ JSON ê°ì²´ ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    if not text:
        return None

    # JSON ë¸”ë¡ ì°¾ê¸° íŒ¨í„´ë“¤
    patterns = [
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡: ```json ... ```
        (r"```json\s*\n?(.*?)\n?```", 1),
        # ì¼ë°˜ ì½”ë“œ ë¸”ë¡: ``` ... ```
        (r"```\s*\n?(.*?)\n?```", 1),
        # JSON ê°ì²´ ì§ì ‘ ë§¤ì¹­: { ... }
        (r"(\{.*\})", 0),
    ]

    import re

    for pattern, group_idx in patterns:
        match = re.search(pattern, text, re.DOTALL)
        if match:
            try:
                json_str = match.group(group_idx if group_idx > 0 else 0)
                return json.loads(json_str)
            except json.JSONDecodeError:
                continue

    # íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ JSONìœ¼ë¡œ íŒŒì‹± ì‹œë„
    try:
        return json.loads(text.strip())
    except json.JSONDecodeError:
        return None


def parse_agent_response(response_text: str, agent_name: str) -> Dict[str, Any]:
    """ì—ì´ì „íŠ¸ ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜

    ê° ì—ì´ì „íŠ¸ì˜ JSON ì‘ë‹µì„ íŒŒì‹±í•˜ê³ , íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        response_text: ì—ì´ì „íŠ¸ì˜ ì›ë³¸ ì‘ë‹µ í…ìŠ¤íŠ¸
        agent_name: ì—ì´ì „íŠ¸ ì´ë¦„ (action_master, pro_proof, context_guardian, quality_consensus)

    Returns:
        Dict[str, Any]: íŒŒì‹±ëœ ì‘ë‹µ ë°ì´í„°
            - action_master, pro_proof, context_guardian: {category, score, reasoning}
            - quality_consensus: {final_evaluation, essential_condition_met, summary_feedback, integrated_improvement}
    """
    parsed_json = extract_json_from_text(response_text)

    if agent_name == "quality_consensus":
        # quality_consensusëŠ” ë‹¤ë¥¸ JSON êµ¬ì¡°ë¥¼ ì‚¬ìš©
        if parsed_json:
            logger.info(f"Successfully parsed JSON from {agent_name}")
            return {
                "final_evaluation": parsed_json.get("final_evaluation", {}),
                "essential_condition_met": parsed_json.get(
                    "essential_condition_met", True
                ),
                "summary_feedback": parsed_json.get("summary_feedback", ""),
                "integrated_improvement": parsed_json.get("integrated_improvement", ""),
                "raw_response": response_text,
            }
        else:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            logger.warning(
                f"Failed to parse JSON from {agent_name}, using default values"
            )
            return {
                "final_evaluation": {
                    "grade": "C",
                    "total_score": 6,
                    "breakdown": {"actionability": 2, "expertise": 2, "context_fit": 2},
                },
                "essential_condition_met": False,
                "summary_feedback": "JSON íŒŒì‹± ì‹¤íŒ¨",
                "integrated_improvement": "ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜ë¡œ í‰ê°€ ë¶ˆê°€",
                "raw_response": response_text,
            }
    else:
        # action_master, pro_proof, context_guardian
        if parsed_json:
            logger.info(
                f"Successfully parsed JSON from {agent_name}: score={parsed_json.get('score', 0)}"
            )
            return {
                "category": parsed_json.get("category", "ì•Œ ìˆ˜ ì—†ìŒ"),
                "score": parsed_json.get("score", 0),
                "reasoning": parsed_json.get("reasoning", ""),
                "raw_response": response_text,
            }
        else:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            logger.warning(
                f"Failed to parse JSON from {agent_name}, using default values"
            )
            return {
                "category": "ì•Œ ìˆ˜ ì—†ìŒ",
                "score": 0,
                "reasoning": "JSON íŒŒì‹± ì‹¤íŒ¨",
                "raw_response": response_text,
            }


def _extract_agent_response(node) -> Dict[str, Any]:
    """ì—ì´ì „íŠ¸ ë…¸ë“œì—ì„œ ì‘ë‹µ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜

    Args:
        node: ê·¸ë˜í”„ ì‹¤í–‰ ê²°ê³¼ì˜ ë…¸ë“œ ê°ì²´

    Returns:
        Dict[str, Any]: agent_name, response_text, parsed_data, execution_time, token_usageë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
    """
    node_id = node.node_id
    text = "(ì‘ë‹µ ì—†ìŒ)"
    execution_time = 0.0
    usage = {}

    if hasattr(node, "result") and node.result:
        agent_result = node.result.result
        if hasattr(agent_result, "message") and agent_result.message:
            content = agent_result.message.get("content", [])
            if content and len(content) > 0:
                text = content[0].get("text", "")

        execution_time = node.result.execution_time / 1000  # ms -> s ë³€í™˜
        usage = getattr(node.result, "accumulated_usage", {})

    # JSON íŒŒì‹± ì¶”ê°€
    parsed_data = parse_agent_response(text, node_id)

    return {
        "agent_name": node_id,
        "response_text": text,
        "parsed_data": parsed_data,
        "execution_time": execution_time,
        "token_usage": usage,
    }


def run_multi_agent_evaluation(
    question_title: str, question_content: str, answer: str
) -> Dict[str, Any]:
    """ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ ì‹¤í–‰í•˜ì—¬ í‰ê°€ ê²°ê³¼ë¥¼ ë°˜í™˜

    ë©˜í‹°ì˜ ì§ˆë¬¸(ì œëª©+ë‚´ìš©)ê³¼ ë©˜í† ì˜ ë‹µë³€ì„ ì…ë ¥ë°›ì•„ 4ê°œ ì—ì´ì „íŠ¸ë¡œ êµ¬ì„±ëœ
    í‰ê°€ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

    Args:
        question_title: ë©˜í‹°ì˜ ì§ˆë¬¸ ì œëª©
        question_content: ë©˜í‹°ì˜ ì§ˆë¬¸ ë‚´ìš©
        answer: ë©˜í† ì˜ ë‹µë³€

    Returns:
        Dict[str, Any]: ê° ì—ì´ì „íŠ¸ì˜ ì‘ë‹µ, ìµœì¢… í•©ì˜, ì‹¤í–‰ ì •ë³´ ë“±ì„ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
    """

    # ì…ë ¥ í¬ë§·íŒ… (ë©˜í‹° ì§ˆë¬¸ê³¼ ë©˜í†  ë‹µë³€ì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜)
    evaluation_input = f"""[ë©˜í‹° ì§ˆë¬¸ ì œëª©]
{question_title}

[ë©˜í‹° ì§ˆë¬¸ ë‚´ìš©]
{question_content}

[ë©˜í†  ë‹µë³€]
{answer}"""

    # ê·¸ë˜í”„ ì‹¤í–‰ (ì—ì´ì „íŠ¸ë“¤ì´ ì •ì˜ëœ ìˆœì„œëŒ€ë¡œ í‰ê°€ ìˆ˜í–‰)
    result = evaluation_graph(evaluation_input)

    # ì—ì´ì „íŠ¸ ì‘ë‹µ ì¶”ì¶œ (í—¬í¼ í•¨ìˆ˜ ì‚¬ìš©)
    agent_responses = [_extract_agent_response(node) for node in result.execution_order]

    # ìµœì¢… í•©ì˜ ê²°ê³¼ (ë§ˆì§€ë§‰ ì—ì´ì „íŠ¸ì¸ quality_consensusì˜ ì‘ë‹µ)
    final_consensus = (
        agent_responses[-1]["response_text"] if agent_responses else "(í‰ê°€ ì‹¤íŒ¨)"
    )

    # ì´ ì‹¤í–‰ ì‹œê°„ ë° í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
    total_execution_time = sum(resp["execution_time"] for resp in agent_responses)
    total_tokens = sum(
        resp["token_usage"].get("totalTokens", 0) for resp in agent_responses
    )

    return {
        "agent_responses": agent_responses,
        "final_consensus": final_consensus,
        "total_execution_time": total_execution_time,
        "total_tokens": total_tokens,
        "execution_order": [node.node_id for node in result.execution_order],
        "status": result.status,
    }


def calculate_grade(score: float, agent_data: Optional[Dict[str, Any]] = None) -> str:
    """DeepEval ì ìˆ˜ (0-10)ë¥¼ D/C/B/A/S ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜

    quality_consensus ì—ì´ì „íŠ¸ì˜ ê³¼ë½ ê·œì¹™ ì ìš©:
    - [ì‹¤í–‰ê°€ëŠ¥ì„±] ë˜ëŠ” [ì „ë¬¸ì„±]ì´ 1ì  ì´í•˜ì¼ ê²½ìš° ìµœëŒ€ Cë“±ê¸‰ìœ¼ë¡œ ì œí•œ

    Args:
        score: DeepEvalì—ì„œ ë°˜í™˜ëœ ì ìˆ˜ (0-10 ë²”ìœ„)
        agent_data: quality_consensusì˜ íŒŒì‹±ëœ ë°ì´í„° (optional)

    Returns:
        str: D, C, B, A, S ì¤‘ í•˜ë‚˜ì˜ ë“±ê¸‰
    """
    # DeepEvalì˜ scoreëŠ” 0-10 ë²”ìœ„
    absolute_score = score

    # ê¸°ë³¸ ë“±ê¸‰ ì‚°ì • (10ì  ë§Œì  ì²´ê³„)
    # D: 0-2, C: 3-4, B: 5-6, A: 7-8, S: 9-10
    if absolute_score >= 9:
        base_grade = "S"
    elif absolute_score >= 7:
        base_grade = "A"
    elif absolute_score >= 5:
        base_grade = "B"
    elif absolute_score >= 3:
        base_grade = "C"
    else:
        base_grade = "D"

    # ê³¼ë½ ê·œì¹™ ì ìš© (quality_consensus ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
    if agent_data and "final_evaluation" in agent_data:
        breakdown = agent_data["final_evaluation"].get("breakdown", {})
        actionability = breakdown.get("actionability", 4)
        expertise = breakdown.get("expertise", 4)

        # ì‹¤í–‰ê°€ëŠ¥ì„± ë˜ëŠ” ì „ë¬¸ì„±ì´ 1ì  ì´í•˜ë©´ Cë“±ê¸‰ìœ¼ë¡œ ì œí•œ
        if actionability <= 1 or expertise <= 1:
            if base_grade in ["S", "A", "B"]:
                return "C"

    return base_grade


# ==================== Phase 2: ë¹„ë™ê¸° í‰ê°€ í•¨ìˆ˜ ====================


async def run_multi_agent_evaluation_async(
    question_title: str, question_content: str, answer: str
) -> Dict[str, Any]:
    """ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰ (Phase 2)

    Strands ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì€ ë™ê¸° ë°©ì‹ì´ë¯€ë¡œ
    ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸°ë¡œ ë˜í•‘í•©ë‹ˆë‹¤.

    Args:
        question_title: ë©˜í‹°ì˜ ì§ˆë¬¸ ì œëª©
        question_content: ë©˜í‹°ì˜ ì§ˆë¬¸ ë‚´ìš©
        answer: ë©˜í† ì˜ ë‹µë³€

    Returns:
        Dict[str, Any]: ê° ì—ì´ì „íŠ¸ì˜ ì‘ë‹µ, ìµœì¢… í•©ì˜, ì‹¤í–‰ ì •ë³´ ë“±ì„ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
    """
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        _agent_executor,
        run_multi_agent_evaluation,
        question_title,
        question_content,
        answer,
    )


async def run_rubric_evaluation_async(
    question_title: str,
    question_content: str,
    answer: str,
    agent_consensus_data: Dict[str, Any],
) -> Dict[str, Any]:
    """DeepEvalì˜ Rubric ê¸°ë°˜ í‰ê°€ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰ (Phase 2)

    DeepEvalì˜ GEval ë©”íŠ¸ë¦­ì˜ ë¹„ë™ê¸° ë©”ì„œë“œ(a_measure)ë¥¼ ì‚¬ìš©í•˜ê³ ,
    ë²ˆì—­ë„ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    Args:
        question_title: ë©˜í‹°ì˜ ì§ˆë¬¸ ì œëª©
        question_content: ë©˜í‹°ì˜ ì§ˆë¬¸ ë‚´ìš©
        answer: ë©˜í† ì˜ ë‹µë³€
        agent_consensus_data: quality_consensus ì—ì´ì „íŠ¸ì˜ íŒŒì‹±ëœ ë°ì´í„° (JSON êµ¬ì¡°)

    Returns:
        Dict[str, Any]: ì ìˆ˜, í•©ê²© ì—¬ë¶€, í‰ê°€ ì´ìœ , ë¹„ìš© ë“±ì„ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
    """
    # quality_consensus ë°ì´í„°ë¥¼ ëª…í™•í•œ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
    final_eval = agent_consensus_data.get("final_evaluation", {})
    breakdown = final_eval.get("breakdown", {})

    # GEvalì´ ì´í•´í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_text = f"""
ë©€í‹° ì—ì´ì „íŠ¸ í‰ê°€ ê²°ê³¼:

[ìµœì¢… ë“±ê¸‰ ë° ì´ì ]
- ë“±ê¸‰: {final_eval.get('grade', 'N/A')}
- ì´ì : {final_eval.get('total_score', 0)}/10ì 

[ì„¸ë¶€ ì ìˆ˜ breakdown]
- ì‹¤í–‰ê°€ëŠ¥ì„± (Actionability): {breakdown.get('actionability', 0)}/4ì 
- ì „ë¬¸ì„± (Expertise): {breakdown.get('expertise', 0)}/4ì 
- í˜„ì‹¤ì„± (Context Fit): {breakdown.get('context_fit', 0)}/2ì 

[í•„ìˆ˜ ì¡°ê±´ ì¶©ì¡± ì—¬ë¶€]
- Essential Condition Met: {agent_consensus_data.get('essential_condition_met', True)}

[ì¢…í•© í‰ê°€]
{agent_consensus_data.get('summary_feedback', '')}

[ê°œì„  ì œì•ˆ]
{agent_consensus_data.get('integrated_improvement', '')}
"""

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„± (ì œëª©ê³¼ ë‚´ìš©ì„ ê²°í•©)
    test_case = LLMTestCase(
        input=f"{question_title}\n\n{question_content}",
        actual_output=answer,
        context=[context_text.strip()],
    )

    # DeepEvalì˜ ë¹„ë™ê¸° ë©”ì„œë“œ ì‚¬ìš©
    await QUALITY_METRIC.a_measure(test_case)

    # ë©”íŠ¸ë¦­ ê²°ê³¼ ì¶”ì¶œ
    score = QUALITY_METRIC.score
    reason = QUALITY_METRIC.reason

    # ë²ˆì—­ë„ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬
    reason_kr = await translate_to_korean_async(reason)

    return {
        "score": score,
        "threshold": QUALITY_METRIC.threshold,
        "success": QUALITY_METRIC.is_successful(),
        "reason_en": reason,
        "reason_kr": reason_kr,
        "evaluation_cost": QUALITY_METRIC.evaluation_cost,
        "evaluation_model": QUALITY_METRIC.evaluation_model,
    }


# ==================== ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì²˜ë¦¬ í•¨ìˆ˜ (Phase 2 ì—…ë°ì´íŠ¸) ====================


async def process_single_test_case(
    test_case: "TestCaseRequest", index: int
) -> "TestResultResponse":
    """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬ (Phase 2 ìµœì í™” ë²„ì „)

    Phase 2 ìµœì í™”:
    - ë¹„ë™ê¸° í•¨ìˆ˜ ì‚¬ìš© (run_multi_agent_evaluation_async, run_rubric_evaluation_async)
    - ThreadPoolExecutorë¥¼ í†µí•œ ìµœì í™”ëœ ìŠ¤ë ˆë“œ ê´€ë¦¬
    - ë²ˆì—­ë„ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬

    Args:
        test_case: í‰ê°€í•  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        index: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¸ë±ìŠ¤

    Returns:
        TestResultResponse: í‰ê°€ ê²°ê³¼

    Raises:
        Exception: í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ
    """
    async with _evaluation_semaphore:
        # Step 1: ë©€í‹° ì—ì´ì „íŠ¸ í‰ê°€ (Phase 2 ë¹„ë™ê¸° í•¨ìˆ˜ ì‚¬ìš©)
        agent_evaluation = await run_multi_agent_evaluation_async(
            test_case.input_title,
            test_case.input_content,
            test_case.actual_output,
        )

        # Step 2: quality_consensus íŒŒì‹±ëœ ë°ì´í„° ì¶”ì¶œ
        quality_consensus_data = None
        for agent_resp in agent_evaluation["agent_responses"]:
            if agent_resp["agent_name"] == "quality_consensus":
                quality_consensus_data = agent_resp.get("parsed_data")
                break

        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
        if not quality_consensus_data:
            quality_consensus_data = {
                "final_evaluation": {
                    "grade": "C",
                    "total_score": 5,
                    "breakdown": {"actionability": 2, "expertise": 2, "context_fit": 1},
                },
                "essential_condition_met": False,
                "summary_feedback": "JSON íŒŒì‹± ì‹¤íŒ¨",
                "integrated_improvement": "ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜ë¡œ í‰ê°€ ë¶ˆê°€",
            }

        # Step 3: Rubric í‰ê°€ (íŒŒì‹±ëœ ë°ì´í„° ì „ë‹¬)
        rubric_evaluation = await run_rubric_evaluation_async(
            test_case.input_title,
            test_case.input_content,
            test_case.actual_output,
            quality_consensus_data,
        )

        # Step 4: ë“±ê¸‰ ì‚°ì • (quality_consensus ë°ì´í„°ë¥¼ í™œìš©í•œ ê³¼ë½ ê·œì¹™ ì ìš©)
        # DeepEvalì˜ scoreëŠ” 0-1 ë²”ìœ„ì´ë¯€ë¡œ 10ì„ ê³±í•´ì„œ 0-10 ë²”ìœ„ë¡œ ë³€í™˜
        normalized_score = rubric_evaluation["score"]  # 0-1 ë²”ìœ„
        absolute_score = normalized_score * 10  # 0-10 ë²”ìœ„ë¡œ ë³€í™˜

        grade = calculate_grade(absolute_score, quality_consensus_data)

        # Step 5: ì‘ë‹µ êµ¬ì„±
        test_result = TestResultResponse(
            test_case_index=index,
            input_title=test_case.input_title,
            input_content=test_case.input_content,
            actual_output=test_case.actual_output,
            expected_output=test_case.expected_output,
            # ê° ì—ì´ì „íŠ¸ì˜ ì‘ë‹µì„ Pydantic ëª¨ë¸ë¡œ ë³€í™˜
            agent_responses=[
                AgentResponseDetail(**agent_resp)
                for agent_resp in agent_evaluation["agent_responses"]
            ],
            final_consensus=agent_evaluation["final_consensus"],
            # Rubric í‰ê°€ ê²°ê³¼ êµ¬ì„±
            rubric_evaluation=RubricEvaluationDetail(
                score=normalized_score,
                absolute_score=absolute_score,
                grade=grade,
                threshold=rubric_evaluation["threshold"],
                success=rubric_evaluation["success"],
                reason=rubric_evaluation["reason_kr"],
                reason_en=rubric_evaluation["reason_en"],
                evaluation_cost=rubric_evaluation["evaluation_cost"],
                evaluation_model=rubric_evaluation["evaluation_model"],
            ),
            # ì‹¤í–‰ ì •ë³´
            total_execution_time=agent_evaluation["total_execution_time"],
            total_tokens=agent_evaluation["total_tokens"],
            execution_order=agent_evaluation["execution_order"],
            success=rubric_evaluation["success"],
        )

        return test_result


# ==================== Pydantic ëª¨ë¸ ì •ì˜ ====================
# FastAPIì˜ ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆë¥¼ ì •ì˜í•˜ëŠ” ëª¨ë¸ë“¤


class TestCaseRequest(BaseModel):
    """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìš”ì²­ ëª¨ë¸"""

    input_title: str  # ë©˜í‹° ì§ˆë¬¸ ì œëª©
    input_content: str  # ë©˜í‹° ì§ˆë¬¸ ë‚´ìš©
    actual_output: str  # ë©˜í†  ë‹µë³€
    expected_output: Optional[str] = None  # ê¸°ëŒ€ ë‹µë³€ (ì„ íƒì‚¬í•­, í˜„ì¬ ë¯¸ì‚¬ìš©)


class EvaluationRequest(BaseModel):
    """í‰ê°€ ìš”ì²­ ëª¨ë¸ (ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ í¬í•¨)"""

    test_cases: List[TestCaseRequest]


class AgentResponseDetail(BaseModel):
    """ê°œë³„ ì—ì´ì „íŠ¸ì˜ ì‘ë‹µ ìƒì„¸ ì •ë³´"""

    agent_name: str  # ì—ì´ì „íŠ¸ ì´ë¦„
    response_text: str  # ì—ì´ì „íŠ¸ ì‘ë‹µ í…ìŠ¤íŠ¸
    parsed_data: Dict[str, Any]  # íŒŒì‹±ëœ JSON ë°ì´í„° (category, score, reasoning ë“±)
    execution_time: float  # ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
    token_usage: Dict[
        str, int
    ]  # í† í° ì‚¬ìš©ëŸ‰ (totalTokens, inputTokens, outputTokens ë“±)


class RubricEvaluationDetail(BaseModel):
    """Rubric ê¸°ë°˜ í‰ê°€ ìƒì„¸ ì •ë³´"""

    score: float  # ì •ê·œí™” ì ìˆ˜ (0-1 ë²”ìœ„, DeepEval ì›ë³¸ ì ìˆ˜)
    absolute_score: float  # ì ˆëŒ€ ì ìˆ˜ (0-10 ë²”ìœ„, score Ã— 10)
    grade: str  # D, C, B, A, S ë“±ê¸‰
    threshold: float  # í•©ê²© ê¸°ì¤€ì  (0-1 ë²”ìœ„, 0.5 = 5ì /10ì )
    success: bool  # í•©ê²© ì—¬ë¶€
    reason: str  # í‰ê°€ ì´ìœ  (í•œê¸€)
    reason_en: str  # í‰ê°€ ì´ìœ  (ì˜ë¬¸ ì›ë³¸)
    evaluation_cost: float  # í‰ê°€ ë¹„ìš©
    evaluation_model: str  # í‰ê°€ì— ì‚¬ìš©ëœ ëª¨ë¸ ì´ë¦„


class TestResultResponse(BaseModel):
    """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ í‰ê°€ ê²°ê³¼"""

    test_case_index: int  # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¸ë±ìŠ¤
    input_title: str  # ë©˜í‹° ì§ˆë¬¸ ì œëª©
    input_content: str  # ë©˜í‹° ì§ˆë¬¸ ë‚´ìš©
    actual_output: str  # ë©˜í†  ë‹µë³€
    expected_output: Optional[str]  # ê¸°ëŒ€ ë‹µë³€ (ì„ íƒì‚¬í•­)

    # ë©€í‹° ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼
    agent_responses: List[AgentResponseDetail]  # ê° ì—ì´ì „íŠ¸ì˜ ì‘ë‹µ
    final_consensus: str  # quality_consensusì˜ ìµœì¢… ë¦¬í¬íŠ¸

    # Rubric ê¸°ë°˜ í‰ê°€ ê²°ê³¼
    rubric_evaluation: RubricEvaluationDetail

    # ì‹¤í–‰ ì •ë³´
    total_execution_time: float  # ì´ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
    total_tokens: int  # ì´ í† í° ì‚¬ìš©ëŸ‰
    execution_order: List[str]  # ì—ì´ì „íŠ¸ ì‹¤í–‰ ìˆœì„œ

    # ì „ì²´ ì„±ê³µ ì—¬ë¶€
    success: bool  # Rubric í‰ê°€ í•©ê²© ì—¬ë¶€


class EvaluationResponse(BaseModel):
    """ì „ì²´ í‰ê°€ ì‘ë‹µ (ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ í¬í•¨)"""

    test_results: List[TestResultResponse]


# ==================== FastAPI ì—”ë“œí¬ì¸íŠ¸ ====================


@app.post("/evaluate", response_model=EvaluationResponse)
async def evaluate_test_cases(request: EvaluationRequest, save_to_db: bool = True):
    """ë©˜í† ë§ ë‹µë³€ì„ í‰ê°€í•˜ëŠ” ë©”ì¸ API ì—”ë“œí¬ì¸íŠ¸ (Phase 2: ë¹„ë™ê¸° ìµœì í™” ë²„ì „)

    ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ë‘ ë‹¨ê³„ì˜ í‰ê°€ í”„ë¡œì„¸ìŠ¤ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    1. ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ í†µí•œ ì •ì„±ì  ë¶„ì„
       - action_master: ì‹¤í–‰ ì§€ì¹¨ êµ¬ì²´ì„± í‰ê°€
       - pro_proof: ì‹¤ë¬´ ì „ë¬¸ì„± ê²€ì¦
       - context_guardian: í˜„ì‹¤ì„± ë¶„ì„
       - quality_consensus: ì¢…í•© ë¦¬í¬íŠ¸ ì‘ì„±

    2. DeepEval Rubric ê¸°ë°˜ ì •ëŸ‰ì  ì ìˆ˜ ì‚°ì¶œ
       - ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ 0-10 ìŠ¤ì¼€ì¼ ì ìˆ˜ ì‚°ì¶œ
       - D/C/B/A/S ë“±ê¸‰ ìë™ ì‚°ì •

    Phase 1 ìµœì í™”:
    - ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ 60-80% ì„±ëŠ¥ í–¥ìƒ
    - Semaphoreë¡œ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ (ê¸°ë³¸ 5ê°œ)
    - ì¼ë¶€ ì‹¤íŒ¨ ì‹œì—ë„ ë¶€ë¶„ ê²°ê³¼ ë°˜í™˜

    Phase 2 ìµœì í™”:
    - ë¹„ë™ê¸° í•¨ìˆ˜ ì‚¬ìš© (run_multi_agent_evaluation_async, run_rubric_evaluation_async)
    - ThreadPoolExecutorë¥¼ í†µí•œ ìµœì í™”ëœ ìŠ¤ë ˆë“œ ê´€ë¦¬
    - ë²ˆì—­ë„ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬ (translate_to_korean_async)
    - DeepEvalì˜ a_measure() ë¹„ë™ê¸° ë©”ì„œë“œ í™œìš©

    Args:
        request: í‰ê°€í•  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤ì„ í¬í•¨í•œ ìš”ì²­ ê°ì²´
        save_to_db: í‰ê°€ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)

    Returns:
        EvaluationResponse: ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ í‰ê°€ ê²°ê³¼ë¥¼ í¬í•¨í•œ ì‘ë‹µ ê°ì²´
            - ê° ì—ì´ì „íŠ¸ì˜ ìƒì„¸ ë¶„ì„
            - ìµœì¢… í•©ì˜ ë¦¬í¬íŠ¸
            - Rubric ì ìˆ˜ ë° ë“±ê¸‰
            - ì‹¤í–‰ ì‹œê°„ ë° í† í° ì‚¬ìš©ëŸ‰

    Example:
        Request:
        {
            "test_cases": [
                {
                    "input": "ì£¼ë‹ˆì–´ ê°œë°œìì¸ë° ì½”ë“œ ë¦¬ë·°ë¥¼ ì˜ ë°›ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
                    "actual_output": "ì½”ë“œ ë¦¬ë·°ë¥¼ ì˜ ë°›ìœ¼ë ¤ë©´..."
                }
            ]
        }

        Response:
        {
            "test_results": [
                {
                    "test_case_index": 0,
                    "rubric_evaluation": {
                        "score": 0.85,
                        "grade": "A",
                        ...
                    },
                    ...
                }
            ]
        }
    """

    # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ë³‘ë ¬ ì²˜ë¦¬
    tasks = [process_single_test_case(tc, i) for i, tc in enumerate(request.test_cases)]

    # ë³‘ë ¬ ì‹¤í–‰ (ì¼ë¶€ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # ì—ëŸ¬ ì²˜ë¦¬ (ì¼ë¶€ ì‹¤íŒ¨í•´ë„ ë¶€ë¶„ ê²°ê³¼ ë°˜í™˜)
    processed_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            # ì—ëŸ¬ ë°œìƒ ì‹œ ì—ëŸ¬ ì‘ë‹µ ìƒì„±
            error_result = TestResultResponse(
                test_case_index=i,
                input_title=request.test_cases[i].input_title,
                input_content=request.test_cases[i].input_content,
                actual_output=request.test_cases[i].actual_output,
                expected_output=request.test_cases[i].expected_output,
                agent_responses=[],
                final_consensus=f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(result)}",
                rubric_evaluation=RubricEvaluationDetail(
                    score=0.0,
                    absolute_score=0.0,
                    grade="D",
                    threshold=0.7,
                    success=False,
                    reason=f"í‰ê°€ ì‹¤íŒ¨: {str(result)}",
                    reason_en=f"Evaluation failed: {str(result)}",
                    evaluation_cost=0.0,
                    evaluation_model="N/A",
                ),
                total_execution_time=0.0,
                total_tokens=0,
                execution_order=[],
                success=False,
            )
            processed_results.append(error_result)
        else:
            processed_results.append(result)

    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ì˜µì…˜)
    if save_to_db:
        with Session(engine) as session:
            for i, result in enumerate(processed_results):
                if not isinstance(result, Exception) and result.success:
                    try:
                        save_evaluation_to_db(session, request.test_cases[i], result)
                        logger.info(f"Saved evaluation result {i} to database")
                    except Exception as e:
                        logger.error(f"Failed to save evaluation {i} to database: {e}")

    return {"test_results": processed_results}


@app.get("/")
def root():
    """API ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸

    API ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì¸ì§€ í™•ì¸í•˜ëŠ” í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤.

    Returns:
        dict: API ìƒíƒœ ë©”ì‹œì§€
    """
    return {"message": "CoEval API is running"}


# ==================== ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸ ====================


@app.get("/evaluations/{evaluation_id}")
def get_evaluation(evaluation_id: int):
    """íŠ¹ì • í‰ê°€ ê²°ê³¼ë¥¼ IDë¡œ ì¡°íšŒí•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸

    Args:
        evaluation_id: ì¡°íšŒí•  í‰ê°€ ë ˆì½”ë“œ ID

    Returns:
        EvaluationRecord: í‰ê°€ ë ˆì½”ë“œ

    Raises:
        HTTPException: í•´ë‹¹ IDì˜ ë ˆì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° 404 ì˜¤ë¥˜
    """
    with Session(engine) as session:
        record = get_evaluation_by_id(session, evaluation_id)
        if not record:
            raise HTTPException(
                status_code=404, detail=f"Evaluation {evaluation_id} not found"
            )
        return record


@app.get("/evaluations")
def list_evaluations(skip: int = 0, limit: int = 100, grade: Optional[str] = None):
    """ëª¨ë“  í‰ê°€ ê²°ê³¼ë¥¼ ì¡°íšŒí•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸ (í˜ì´ì§€ë„¤ì´ì…˜ ì§€ì›)

    Args:
        skip: ê±´ë„ˆë›¸ ë ˆì½”ë“œ ìˆ˜ (ê¸°ë³¸ê°’: 0)
        limit: ìµœëŒ€ ë°˜í™˜ ë ˆì½”ë“œ ìˆ˜ (ê¸°ë³¸ê°’: 100)
        grade: ë“±ê¸‰ í•„í„° (S, A, B, C, D ì¤‘ í•˜ë‚˜, ì„ íƒì‚¬í•­)

    Returns:
        List[EvaluationRecord]: í‰ê°€ ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸

    Example:
        GET /evaluations?skip=0&limit=10&grade=S
        - Së“±ê¸‰ í‰ê°€ ê²°ê³¼ ì¤‘ ìµœì‹  10ê°œ ì¡°íšŒ
    """
    with Session(engine) as session:
        records = get_all_evaluations(session, skip=skip, limit=limit, grade=grade)
        return records


@app.get("/evaluations/score-range")
def get_evaluations_by_score(min_score: float = 0.0, max_score: float = 10.0):
    """ì ìˆ˜ ë²”ìœ„ë¡œ í‰ê°€ ê²°ê³¼ë¥¼ ì¡°íšŒí•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸

    Args:
        min_score: ìµœì†Œ ì ìˆ˜ (0-10 ë²”ìœ„, ê¸°ë³¸ê°’: 0.0)
        max_score: ìµœëŒ€ ì ìˆ˜ (0-10 ë²”ìœ„, ê¸°ë³¸ê°’: 10.0)

    Returns:
        List[EvaluationRecord]: í‰ê°€ ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸

    Example:
        GET /evaluations/score-range?min_score=7.0&max_score=10.0
        - 7ì  ì´ìƒ 10ì  ì´í•˜ì˜ í‰ê°€ ê²°ê³¼ ì¡°íšŒ (Aë“±ê¸‰ ì´ìƒ)
    """
    with Session(engine) as session:
        records = get_evaluations_by_score_range(session, min_score, max_score)
        return records


@app.delete("/evaluations/{evaluation_id}")
def delete_evaluation_endpoint(evaluation_id: int):
    """íŠ¹ì • í‰ê°€ ê²°ê³¼ë¥¼ ì‚­ì œí•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸

    Args:
        evaluation_id: ì‚­ì œí•  í‰ê°€ ë ˆì½”ë“œ ID

    Returns:
        dict: ì‚­ì œ ê²°ê³¼ ë©”ì‹œì§€

    Raises:
        HTTPException: í•´ë‹¹ IDì˜ ë ˆì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° 404 ì˜¤ë¥˜
    """
    with Session(engine) as session:
        success = delete_evaluation(session, evaluation_id)
        if not success:
            raise HTTPException(
                status_code=404, detail=f"Evaluation {evaluation_id} not found"
            )
        return {"message": f"Evaluation {evaluation_id} deleted successfully"}


@app.get("/statistics")
def get_evaluation_statistics():
    """í‰ê°€ í†µê³„ë¥¼ ë°˜í™˜í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸

    Returns:
        dict: í‰ê°€ í†µê³„
            - total_evaluations: ì´ í‰ê°€ ìˆ˜
            - grade_distribution: ë“±ê¸‰ë³„ ë¶„í¬ (S, A, B, C, D)
            - average_score: í‰ê·  ì ìˆ˜
            - success_rate: í•©ê²©ë¥  (%)

    Example:
        GET /statistics
        {
            "total_evaluations": 150,
            "grade_distribution": {"S": 20, "A": 40, "B": 50, "C": 30, "D": 10},
            "average_score": 6.5,
            "success_rate": 73.3
        }
    """
    with Session(engine) as session:
        all_records = get_all_evaluations(session, skip=0, limit=10000)

        if not all_records:
            return {
                "total_evaluations": 0,
                "grade_distribution": {"S": 0, "A": 0, "B": 0, "C": 0, "D": 0},
                "average_score": 0.0,
                "success_rate": 0.0,
            }

        total = len(all_records)
        grade_dist = {"S": 0, "A": 0, "B": 0, "C": 0, "D": 0}
        total_score = 0.0
        success_count = 0

        for record in all_records:
            grade_dist[record.grade] = grade_dist.get(record.grade, 0) + 1
            total_score += record.total_score
            if record.success:
                success_count += 1

        return {
            "total_evaluations": total,
            "grade_distribution": grade_dist,
            "average_score": round(total_score / total, 2) if total > 0 else 0.0,
            "success_rate": (
                round((success_count / total) * 100, 2) if total > 0 else 0.0
            ),
        }
