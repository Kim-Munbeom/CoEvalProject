"""
CoEval: ë©˜í† ë§ ë‹µë³€ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œê³¼ DeepEval Rubric ê¸°ë°˜ í‰ê°€ë¥¼ ê²°í•©í•˜ì—¬
ë©˜í† ë§ ë‹µë³€ì˜ í’ˆì§ˆì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ í†µí•œ ë‹¤ê°ë„ í‰ê°€ (ì‹¤í–‰ê°€ëŠ¥ì„±, ì „ë¬¸ì„±, í˜„ì‹¤ì„±)
- DeepEval Rubric ê¸°ë°˜ ì •ëŸ‰ì  ì ìˆ˜ ì‚°ì¶œ (0-10 ìŠ¤ì¼€ì¼)
- ë“±ê¸‰ ì²´ê³„ (D/C/B/A/S) ìë™ ì‚°ì • ë° ê³¼ë½ ê·œì¹™ ì ìš©
- JSON ê¸°ë°˜ êµ¬ì¡°í™”ëœ ì—ì´ì „íŠ¸ ì‘ë‹µ íŒŒì‹±
- í‰ê°€ ì´ìœ  í•œê¸€ ë²ˆì—­ ì œê³µ
- ìƒì„¸ ë¡œê¹… ë° ì—ëŸ¬ í•¸ë“¤ë§

ì ìˆ˜ ì²´ê³„:
- ì—ì´ì „íŠ¸ í‰ê°€: ì‹¤í–‰ê°€ëŠ¥ì„± 0-4ì  + ì „ë¬¸ì„± 0-4ì  + í˜„ì‹¤ì„± 0-2ì  = 10ì  ë§Œì 
- DeepEval í‰ê°€: ì—ì´ì „íŠ¸ 10ì ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ Rubric í‰ê°€ (0-10 ë²”ìœ„)
- ë“±ê¸‰ ê¸°ì¤€: D(0-2), C(3-4), B(5-6), A(7-8), S(9-10)

ë²„ì „ ê°œì„ ì‚¬í•­:
- ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ JSON ì „ìš© ì¶œë ¥ìœ¼ë¡œ ë³€ê²½
- 10ì  ë§Œì  ì²´ê³„ë¡œ í†µì¼ (actionability: 0-4, expertise: 0-4, context_fit: 0-2)
- quality_consensusì˜ ê³¼ë½ ê·œì¹™ (ì‹¤í–‰ê°€ëŠ¥ì„± ë˜ëŠ” ì „ë¬¸ì„± â‰¤ 1ì ) ì ìš©
- êµ¬ì¡°í™”ëœ parsed_data í•„ë“œ ì¶”ê°€ë¡œ ì—ì´ì „íŠ¸ ì ìˆ˜ ì ‘ê·¼ì„± í–¥ìƒ
"""

import asyncio
import concurrent.futures
import json
import logging
import os
from typing import List, Optional, Dict, Any

from deepeval.metrics import GEval
from deepeval.metrics.g_eval import Rubric
from deepeval.models import GeminiModel as DeepEvalGeminiModel
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from dotenv import load_dotenv
from fastapi import FastAPI
from google import genai
from pydantic import BaseModel
from strands import Agent
from strands.models.gemini import GeminiModel as StrandsGeminiModel
from strands.multiagent import GraphBuilder

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ì—ì„œ API í‚¤ ë“± ë¡œë“œ)
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
app = FastAPI()

# Strandsìš© Gemini ëª¨ë¸ (ë©€í‹° ì—ì´ì „íŠ¸ìš©)
# ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì—ì„œ ê° ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•  LLM ëª¨ë¸
# temperature: 0.3ìœ¼ë¡œ ë‚®ì¶° ì¼ê´€ì„± ìˆëŠ” í‰ê°€ ê²°ê³¼ ìœ ë„
strands_gemini_model = StrandsGeminiModel(
    client_args={
        "api_key": os.getenv("GEMINI_API_KEY"),
    },
    model_id="gemini-2.5-flash",
    params={
        "temperature": 0.3,  # ë‚®ì€ ì˜¨ë„ë¡œ ì¼ê´€ëœ í‰ê°€
        "max_output_tokens": 8192,  # ê¸´ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ê°€ëŠ¥
        "top_p": 0.6,
        "top_k": 20,
    },
)

# DeepEvalìš© Gemini ëª¨ë¸ (Rubric í‰ê°€ìš©)
# Rubric ê¸°ë°˜ ì •ëŸ‰ì  ì ìˆ˜ ì‚°ì¶œì— ì‚¬ìš©
deepeval_gemini_model = DeepEvalGeminiModel(
    model="gemini-2.5-flash", api_key=os.getenv("GEMINI_API_KEY"), temperature=0.3
)

# Google GenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ë²ˆì—­ìš©)
# DeepEvalì˜ ì˜ë¬¸ í‰ê°€ ì´ìœ ë¥¼ í•œê¸€ë¡œ ë²ˆì—­í•˜ê¸° ìœ„í•´ ì‚¬ìš©
genai_client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))


async def translate_to_korean_async(text: str) -> str:
    """í‰ê°€ ì´ìœ ë¥¼ í•œê¸€ë¡œ ë²ˆì—­í•˜ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜

    ë™ê¸° ë²ˆì—­ í•¨ìˆ˜ë¥¼ executorë¡œ ë˜í•‘í•˜ì—¬ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.

    Args:
        text: ë²ˆì—­í•  ì˜ë¬¸ í…ìŠ¤íŠ¸

    Returns:
        str: ë²ˆì—­ëœ í•œê¸€ í…ìŠ¤íŠ¸ (ì‹¤íŒ¨ ì‹œ ì›ë¬¸)
    """

    def _generate_translation() -> str:
        """ë™ê¸°ì ìœ¼ë¡œ ë²ˆì—­ì„ ìˆ˜í–‰í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜"""
        response = genai_client.models.generate_content(
            model="gemini-2.5-flash-lite",
            contents=f"ë‹¤ìŒ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ë²ˆì—­ë¬¸ë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”:\n\n{text}",
        )
        return response.text.strip()

    try:
        loop = asyncio.get_event_loop()
        response_text = await loop.run_in_executor(None, _generate_translation)
        return response_text
    except (ValueError, RuntimeError, ConnectionError) as e:
        # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ë°˜í™˜ (êµ¬ì²´ì ì¸ ì˜ˆì™¸ ì²˜ë¦¬)
        logger.warning(f"Translation failed: {e}")
        return text


# ì—ì´ì „íŠ¸ ì„¤ì • ë°ì´í„° (ë°ì´í„° ê¸°ë°˜ êµ¬ì„±ìœ¼ë¡œ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ)
AGENT_CONFIGS = {
    "action_master": {
        "description": "ì‹¤í–‰ê°€ëŠ¥ì„± ë¶„ì„ê°€(Actionability Expert)",
        "system_prompt": """# Role
ë‹¹ì‹ ì€ Q&A í”Œë«í¼ì˜ 'ì‹¤í–‰ê°€ëŠ¥ì„± ë¶„ì„ê°€'ì…ë‹ˆë‹¤.

# Evaluation Criteria: [êµ¬ì²´ì„± & ì‹¤í–‰ ê°€ëŠ¥ì„±] (0~4ì )
* **4ì  (íƒì›”):** êµ¬ì²´ì  í–‰ë™ ì§€ì¹¨(Step-by-step), ìˆ˜ì¹˜, ë„êµ¬ ë“±ì´ ì™„ë²½í•˜ì—¬ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë‹¤.
* **3ì  (ìš°ìˆ˜):** ì‹¤í–‰ ë°©ë²•ì€ êµ¬ì²´ì ì´ë‚˜, ì˜ˆì‹œë‚˜ ì‚¬ì†Œí•œ ë””í…Œì¼ì´ í•˜ë‚˜ ì •ë„ ë¶€ì¡±í•˜ë‹¤.
* **2ì  (ë³´í†µ):** ë°©í–¥ì€ ë§ìœ¼ë‚˜ 'ì–´ë–»ê²Œ'ì— ëŒ€í•œ ì„¤ëª…ì´ ë‹¤ì†Œ ì¼ë°˜ì ì´ë‹¤.
* **1ì  (ë¯¸í¡):** ì¶”ìƒì ì¸ ì¡°ì–¸ ìœ„ì£¼ë¼ ë¬´ì—‡ë¶€í„° í•´ì•¼ í• ì§€ ë§‰ë§‰í•˜ë‹¤.
* **0ì  (ë¬´ì˜ë¯¸):** ì‹¤í–‰ ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜ ë‚´ìš©ì´ ì—†ë‹¤.

# Output Instruction (JSON Only)
{
  "category": "actionability",
  "score": (0~4 ì •ìˆ˜),
  "reasoning": "(í•µì‹¬ ê·¼ê±° 1ë¬¸ì¥)"
}""",
    },
    "pro_proof": {
        "description": "ì§ë¬´ ì „ë¬¸ê°€(Domain Expert)",
        "system_prompt": """# Role
ë‹¹ì‹ ì€ í•´ë‹¹ ì—…ê³„ì˜ 'ì§ë¬´ ì „ë¬¸ê°€'ì…ë‹ˆë‹¤.

# Evaluation Criteria: [ì „ë¬¸ì„± & ê²½í—˜] (0~4ì )
* **4ì  (íƒì›”):** í˜„ì—… ìš©ì–´/í”„ë¡œì„¸ìŠ¤ê°€ ì™„ë²½í•˜ë©°, ì‹¤ì œ ê²½í—˜ ê¸°ë°˜ì˜ ê¹Šì€ ì¸ì‚¬ì´íŠ¸ê°€ ìˆë‹¤.
* **3ì  (ìš°ìˆ˜):** ì •í™•í•œ ì‹¤ë¬´ ì§€ì‹ê³¼ ë„êµ¬ë¥¼ ë‹¤ë£¨ê³  ìˆìœ¼ë‚˜, ê³ ìœ í•œ ê²½í—˜ë³´ë‹¤ëŠ” ì •ë³´ ì „ë‹¬ ìœ„ì£¼ë‹¤.
* **2ì  (ë³´í†µ):** ê²€ìƒ‰í•˜ë©´ ë‚˜ì˜¤ëŠ” ì¼ë°˜ì ì¸ ì§€ì‹ ìˆ˜ì¤€ì´ë‹¤. í‹€ë¦° ë‚´ìš©ì€ ì—†ë‹¤.
* **1ì  (ë¯¸í¡):** ì „ë¬¸ ìš©ì–´ê°€ ì–´ìƒ‰í•˜ê±°ë‚˜ ë¹„ì „ë¬¸ê°€ë„ í•  ìˆ˜ ìˆëŠ” ì–•ì€ ì¡°ì–¸ì´ë‹¤.
* **0ì  (ë¬´ì˜ë¯¸):** ì „ë¬¸ì„±ì´ ì—†ê±°ë‚˜ ì˜ëª»ëœ ì •ë³´ë‹¤.

# Output Instruction (JSON Only)
{
  "category": "expertise",
  "score": (0~4 ì •ìˆ˜),
  "reasoning": "(í•µì‹¬ ê·¼ê±° 1ë¬¸ì¥)"
}""",
    },
    "context_guardian": {
        "description": "í˜„ì‹¤ì„± ë¶„ì„ê°€(Context Analyst)",
        "system_prompt": """# Role
ë‹¹ì‹ ì€ ë©˜í‹°ì˜ ìƒí™©ì„ íŒŒì•…í•˜ëŠ” 'í˜„ì‹¤ì„± ë¶„ì„ê°€'ì…ë‹ˆë‹¤.

# Evaluation Criteria: [í˜„ì‹¤ì„± & ë§¥ë½ ì í•©ì„±] (0~2ì )
* **2ì  (ì í•©):** ë©˜í‹°ì˜ ìƒí™©/ì—°ì°¨ë¥¼ ê³ ë ¤í–ˆìœ¼ë©°, í˜„ì‹¤ì ì¸ ì œì•½ì´ë‚˜ ì£¼ì˜ì (Risk)ê¹Œì§€ ì§šì–´ì£¼ì—ˆë‹¤.
* **1ì  (ë³´í†µ):** ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì€ ë˜ì§€ë§Œ, ë©˜í‹°ì˜ êµ¬ì²´ì  ìƒí™©ë³´ë‹¤ëŠ” ì¼ë°˜ë¡ ì— ê°€ê¹ë‹¤.
* **0ì  (ë¶€ì í•©):** ë©˜í‹° ìƒí™©ê³¼ ë§ì§€ ì•Šê±°ë‚˜ ë³µì‚¬ ë¶™ì—¬ë„£ê¸° ì‹ ë‹µë³€ì´ë‹¤.

# Output Instruction (JSON Only)
{
  "category": "context_fit",
  "score": (0~2 ì •ìˆ˜),
  "reasoning": "(í•µì‹¬ ê·¼ê±° 1ë¬¸ì¥)"
}""",
    },
    "quality_consensus": {
        "description": "ì¢…í•© í‰ê°€ ìœ„ì›ì¥(Master Judge)",
        "system_prompt": """# Role
ë‹¹ì‹ ì€ 'ì¢…í•© í‰ê°€ ìœ„ì›ì¥'ì…ë‹ˆë‹¤. 3ëª…ì˜ ë¶„ì„ê°€ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë“±ê¸‰(10ì  ë§Œì )ì„ ë§¤ê¸°ê³  í†µí•© í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.

# Scoring Rules (ì´ 10ì  ë§Œì )
* **ì´ì :** Agent 1(4ì ) + Agent 2(4ì ) + Agent 3(2ì ) í•©ê³„

# Grading System (5 Grades)
* **Së“±ê¸‰ (9~10ì ):** ì™„ë²½ì— ê°€ê¹Œìš´ ë‹µë³€. (ì¦‰ì‹œ ì±„íƒ ê¶Œì¥)
* **Aë“±ê¸‰ (7~8ì ):** í›Œë¥­í•œ ë‹µë³€. (ë””í…Œì¼ ë³´ì™„ ì‹œ ì™„ë²½)
* **Bë“±ê¸‰ (5~6ì ):** í‰ë²”í•œ ë‹µë³€. (ë„ì›€ì€ ë˜ë‚˜ ê¹Šì´ê°€ ë¶€ì¡±)
* **Cë“±ê¸‰ (3~4ì ):** ì•„ì‰¬ìš´ ë‹µë³€. (í•µì‹¬ ìš”ì†Œ ê²°ì—¬)
* **Dë“±ê¸‰ (0~2ì ):** ë„ì›€ ë˜ì§€ ì•ŠìŒ.

# ğŸ”’ Essential Conditions (ê³¼ë½)
* [ì‹¤í–‰ê°€ëŠ¥ì„±]ì´ë‚˜ [ì „ë¬¸ì„±] ì¤‘ í•˜ë‚˜ë¼ë„ **1ì  ì´í•˜**ì¼ ê²½ìš°, ì´ì ì´ ì•„ë¬´ë¦¬ ë†’ì•„ë„ ìµœëŒ€ ë“±ê¸‰ì€ **Cë“±ê¸‰**ìœ¼ë¡œ ì œí•œë©ë‹ˆë‹¤.

# Input Data
- Agent 1, 2, 3ì˜ JSON ê²°ê³¼
- ë©˜í† ì˜ ì›ë³¸ ë‹µë³€

# Output Format (JSON)
{
  "final_evaluation": {
    "grade": "S/A/B/C/D",
    "total_score": (0~10 ì •ìˆ˜),
    "breakdown": {
      "actionability": (0~4 ì ìˆ˜),
      "expertise": (0~4 ì ìˆ˜),
      "context_fit": (0~2 ì ìˆ˜)
    }
  },
  "essential_condition_met": true/false,
  "summary_feedback": "(ë‹µë³€ì˜ ì¥ì ì„ ìš”ì•½í•œ í•œ ë¬¸ì¥)",
  "integrated_improvement": "(ë“±ê¸‰ ìƒìŠ¹ì„ ìœ„í•´ ê°€ì¥ ì‹œê¸‰í•˜ê²Œ ë³´ì™„í•´ì•¼ í•  êµ¬ì²´ì  ì¡°ì–¸ 1ê°€ì§€. ì™„ë²½í•˜ë‹¤ë©´ 'ì—†ìŒ' í‘œê¸°)"
}""",
    },
}


def create_evaluation_agents(model: StrandsGeminiModel) -> Dict[str, Agent]:
    """ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ ìƒì„±í•˜ëŠ” íŒ©í† ë¦¬ í•¨ìˆ˜

    AGENT_CONFIGS ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—ì´ì „íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        model: ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•  Gemini ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤

    Returns:
        Dict[str, Agent]: ì—ì´ì „íŠ¸ ì´ë¦„ì„ í‚¤ë¡œ í•˜ëŠ” ì—ì´ì „íŠ¸ ë”•ì…”ë„ˆë¦¬
    """
    return {
        name: Agent(name=name, system_prompt=config["system_prompt"], model=model)
        for name, config in AGENT_CONFIGS.items()
    }


def build_evaluation_graph(agents: Dict[str, Agent]):
    """í‰ê°€ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•˜ëŠ” í•¨ìˆ˜ (ìµœì í™” ë²„ì „)

    ì—ì´ì „íŠ¸ë“¤ì˜ ì‹¤í–‰ ìˆœì„œë¥¼ ì •ì˜í•˜ëŠ” DAG(Directed Acyclic Graph)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    ìµœì í™”ëœ ì‹¤í–‰ íë¦„:
    1. action_master, pro_proof, context_guardianì´ **ëª¨ë‘ ë³‘ë ¬**ë¡œ ì‹¤í–‰
       - ê° ì—ì´ì „íŠ¸ê°€ ë…ë¦½ì ìœ¼ë¡œ í‰ê°€ ìˆ˜í–‰ (ì‹¤í–‰ì„±, ì „ë¬¸ì„±, í˜„ì‹¤ì„±)
    2. quality_consensusê°€ ì„¸ ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë¦¬í¬íŠ¸ ì‘ì„±

    ê¸°ì¡´ ëŒ€ë¹„ ê°œì„ :
    - context_guardianì´ action_master, pro_proof ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ì§€ ì•ŠìŒ
    - 3ê°œ ì—ì´ì „íŠ¸ ë³‘ë ¬ ì‹¤í–‰ìœ¼ë¡œ 10-15% ì¶”ê°€ ì„±ëŠ¥ í–¥ìƒ

    Args:
        agents: ì—ì´ì „íŠ¸ ë”•ì…”ë„ˆë¦¬

    Returns:
        ì‹¤í–‰ ê°€ëŠ¥í•œ ê·¸ë˜í”„ ê°ì²´
    """
    builder = GraphBuilder()

    # ë…¸ë“œ ë“±ë¡ (ê° ì—ì´ì „íŠ¸ë¥¼ ê·¸ë˜í”„ ë…¸ë“œë¡œ ì¶”ê°€)
    for name in AGENT_CONFIGS:
        builder.add_node(agents[name], name)

    # ì—£ì§€ ì •ì˜ (ìµœì í™”: 3ê°œ ì—ì´ì „íŠ¸ ë³‘ë ¬ ì‹¤í–‰)
    # action_master, pro_proof, context_guardian â†’ quality_consensus
    builder.add_edge("action_master", "quality_consensus")
    builder.add_edge("pro_proof", "quality_consensus")
    builder.add_edge("context_guardian", "quality_consensus")

    return builder.build()


# ì „ì—­ ë³€ìˆ˜ë¡œ ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì´ˆê¸°í™”
# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ìƒì„±í•˜ì—¬ ì¬ì‚¬ìš©
evaluation_agents = create_evaluation_agents(strands_gemini_model)
evaluation_graph = build_evaluation_graph(evaluation_agents)

# ==================== Phase 1: ì „ì—­ ë©”íŠ¸ë¦­ ë° ë™ì‹œì„± ì œì–´ ====================

# Rubric ì •ì˜ ì „ì—­í™” (ë§¤ë²ˆ ìƒì„±í•˜ì§€ ì•Šê³  ì¬ì‚¬ìš©)
# ì—ì´ì „íŠ¸ 10ì  ë§Œì  ì²´ê³„ë¥¼ ê·¸ëŒ€ë¡œ DeepEval Rubricì— ë§¤í•‘
MENTORING_RUBRIC = [
    Rubric(
        score_range=(0, 2),
        expected_outcome="Dë“±ê¸‰ (0-2/10ì ): í•„ìˆ˜ ì¡°ê±´ ë¯¸ë‹¬. ì‹¤í–‰ê°€ëŠ¥ì„±/ì „ë¬¸ì„±ì´ ê²°ì—¬ëœ ë‹µë³€. ì¶”ìƒì ì´ê³  ì‹¤í–‰ ë¶ˆê°€ëŠ¥.",
    ),
    Rubric(
        score_range=(3, 4),
        expected_outcome="Cë“±ê¸‰ (3-4/10ì ): ì¡°ì–¸ì€ ìˆìœ¼ë‚˜ ì¶”ìƒì ì´ë©° ë©˜í‹° ìƒí™© ê³ ë ¤ê°€ ë¶€ì¡±í•¨. ì¼ë°˜ì ì¸ ì§€ì‹ ìˆ˜ì¤€.",
    ),
    Rubric(
        score_range=(5, 6),
        expected_outcome="Bë“±ê¸‰ (5-6/10ì ): ì–‘í˜¸í•¨. êµ¬ì²´ì  ë‹¨ê³„ì™€ ì‹¤ë¬´ ì§€ì‹ì´ ì¼ë¶€ í¬í•¨. ì‹¤í–‰ ê°€ëŠ¥í•œ ë°©í–¥ì„± ì œì‹œ.",
    ),
    Rubric(
        score_range=(7, 8),
        expected_outcome="Aë“±ê¸‰ (7-8/10ì ): ìš°ìˆ˜í•¨. êµ¬ì²´ì  ë‹¨ê³„, ì‹¤ë¬´ ì§€ì‹, ë©˜í‹° ë§¥ë½ ê³ ë ¤ê°€ ì˜ ë˜ì–´ìˆìŒ. ë†’ì€ ìˆ˜ì¤€ì˜ ë‹µë³€.",
    ),
    Rubric(
        score_range=(9, 10),
        expected_outcome="Së“±ê¸‰ (9-10/10ì ): ì™„ë²½í•¨. ìˆ˜ì¹˜/ë„êµ¬/ë‹¨ê³„, ì‹¤ë¬´ ê²½í—˜ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸, ë¦¬ìŠ¤í¬ ê´€ë¦¬, ë©˜í‹° ìƒí™© ì™„ë²½ ê³ ë ¤.",
    ),
]

# GEval ë©”íŠ¸ë¦­ ì „ì—­í™”
QUALITY_METRIC = GEval(
    name="Overall Mentoring Quality",
    evaluation_params=[
        LLMTestCaseParams.INPUT,
        LLMTestCaseParams.ACTUAL_OUTPUT,
        LLMTestCaseParams.CONTEXT,
    ],
    evaluation_steps=[
        "1. Contextì— í¬í•¨ëœ quality_consensus ì—ì´ì „íŠ¸ì˜ ì¢…í•© í‰ê°€ë¥¼ í™•ì¸í•œë‹¤. final_evaluationì˜ breakdown(actionability, expertise, context_fit) ì ìˆ˜ë¥¼ ê°ê° ê²€í† í•œë‹¤.",
        "2. ê° í•­ëª©ì˜ ì ìˆ˜ ë²”ìœ„ë¥¼ í™•ì¸í•œë‹¤: actionability(0-4ì ), expertise(0-4ì ), context_fit(0-2ì ), ì´ì  10ì  ë§Œì .",
        "3. ê³¼ë½ ì¡°ê±´ì„ í™•ì¸í•œë‹¤: actionability ë˜ëŠ” expertiseê°€ 1ì  ì´í•˜ì¸ ê²½ìš° ìµœëŒ€ Cë“±ê¸‰ìœ¼ë¡œ ì œí•œí•œë‹¤.",
        "4. ì´ì (10ì  ë§Œì )ì„ Rubric êµ¬ê°„ì— ë§¤í•‘í•œë‹¤: D(0-2), C(3-4), B(5-6), A(7-8), S(9-10)",
        "5. ë©˜í†  ë‹µë³€ì´ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•˜ë©° ì „ë¬¸ì ì¸ì§€, ë©˜í‹° ìƒí™©ì„ ê³ ë ¤í–ˆëŠ”ì§€ ì¢…í•© í‰ê°€í•˜ì—¬ ìµœì¢… ì ìˆ˜ë¥¼ 0-10ì  ë²”ìœ„ë¡œ í™•ì •í•œë‹¤.",
        "6. ì ìˆ˜ ê²°ì • ê·¼ê±°ë¥¼ 1-2ë¬¸ì¥ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ìš”ì•½í•œë‹¤.",
    ],
    rubric=MENTORING_RUBRIC,
    threshold=0.5,  # 10ì  ë§Œì ì—ì„œ 5ì  ì´ìƒì´ë©´ í•©ê²© (Bë“±ê¸‰ ì´ìƒ)
    model=deepeval_gemini_model,
)

# ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ (API Rate Limiting)
MAX_CONCURRENT_EVALUATIONS = 5
_evaluation_semaphore = asyncio.Semaphore(MAX_CONCURRENT_EVALUATIONS)

# ThreadPoolExecutor for async wrapping of sync functions (Phase 2)
_agent_executor = concurrent.futures.ThreadPoolExecutor(max_workers=5)


# ==================== JSON Parsing Utilities ====================


def extract_json_from_text(text: str) -> Optional[Dict[str, Any]]:
    """í…ìŠ¤íŠ¸ì—ì„œ JSONì„ ì¶”ì¶œí•˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜

    ì—ì´ì „íŠ¸ ì‘ë‹µì—ì„œ JSON ë¸”ë¡ì„ ì°¾ì•„ íŒŒì‹±í•©ë‹ˆë‹¤.
    ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì´ë‚˜ ì¼ë°˜ í…ìŠ¤íŠ¸ì—ì„œ JSONì„ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    Args:
        text: íŒŒì‹±í•  í…ìŠ¤íŠ¸

    Returns:
        Dict[str, Any]: íŒŒì‹±ëœ JSON ê°ì²´ ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    if not text:
        return None

    # JSON ë¸”ë¡ ì°¾ê¸° íŒ¨í„´ë“¤
    patterns = [
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡: ```json ... ```
        (r"```json\s*\n?(.*?)\n?```", 1),
        # ì¼ë°˜ ì½”ë“œ ë¸”ë¡: ``` ... ```
        (r"```\s*\n?(.*?)\n?```", 1),
        # JSON ê°ì²´ ì§ì ‘ ë§¤ì¹­: { ... }
        (r"(\{.*\})", 0),
    ]

    import re

    for pattern, group_idx in patterns:
        match = re.search(pattern, text, re.DOTALL)
        if match:
            try:
                json_str = match.group(group_idx if group_idx > 0 else 0)
                return json.loads(json_str)
            except json.JSONDecodeError:
                continue

    # íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ JSONìœ¼ë¡œ íŒŒì‹± ì‹œë„
    try:
        return json.loads(text.strip())
    except json.JSONDecodeError:
        return None


def parse_agent_response(response_text: str, agent_name: str) -> Dict[str, Any]:
    """ì—ì´ì „íŠ¸ ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜

    ê° ì—ì´ì „íŠ¸ì˜ JSON ì‘ë‹µì„ íŒŒì‹±í•˜ê³ , íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        response_text: ì—ì´ì „íŠ¸ì˜ ì›ë³¸ ì‘ë‹µ í…ìŠ¤íŠ¸
        agent_name: ì—ì´ì „íŠ¸ ì´ë¦„ (action_master, pro_proof, context_guardian, quality_consensus)

    Returns:
        Dict[str, Any]: íŒŒì‹±ëœ ì‘ë‹µ ë°ì´í„°
            - action_master, pro_proof, context_guardian: {category, score, reasoning}
            - quality_consensus: {final_evaluation, essential_condition_met, summary_feedback, integrated_improvement}
    """
    parsed_json = extract_json_from_text(response_text)

    if agent_name == "quality_consensus":
        # quality_consensusëŠ” ë‹¤ë¥¸ JSON êµ¬ì¡°ë¥¼ ì‚¬ìš©
        if parsed_json:
            logger.info(f"Successfully parsed JSON from {agent_name}")
            return {
                "final_evaluation": parsed_json.get("final_evaluation", {}),
                "essential_condition_met": parsed_json.get(
                    "essential_condition_met", True
                ),
                "summary_feedback": parsed_json.get("summary_feedback", ""),
                "integrated_improvement": parsed_json.get("integrated_improvement", ""),
                "raw_response": response_text,
            }
        else:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            logger.warning(
                f"Failed to parse JSON from {agent_name}, using default values"
            )
            return {
                "final_evaluation": {
                    "grade": "C",
                    "total_score": 6,
                    "breakdown": {"actionability": 2, "expertise": 2, "context_fit": 2},
                },
                "essential_condition_met": False,
                "summary_feedback": "JSON íŒŒì‹± ì‹¤íŒ¨",
                "integrated_improvement": "ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜ë¡œ í‰ê°€ ë¶ˆê°€",
                "raw_response": response_text,
            }
    else:
        # action_master, pro_proof, context_guardian
        if parsed_json:
            logger.info(
                f"Successfully parsed JSON from {agent_name}: score={parsed_json.get('score', 0)}"
            )
            return {
                "category": parsed_json.get("category", "ì•Œ ìˆ˜ ì—†ìŒ"),
                "score": parsed_json.get("score", 0),
                "reasoning": parsed_json.get("reasoning", ""),
                "raw_response": response_text,
            }
        else:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            logger.warning(
                f"Failed to parse JSON from {agent_name}, using default values"
            )
            return {
                "category": "ì•Œ ìˆ˜ ì—†ìŒ",
                "score": 0,
                "reasoning": "JSON íŒŒì‹± ì‹¤íŒ¨",
                "raw_response": response_text,
            }


def _extract_agent_response(node) -> Dict[str, Any]:
    """ì—ì´ì „íŠ¸ ë…¸ë“œì—ì„œ ì‘ë‹µ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜

    Args:
        node: ê·¸ë˜í”„ ì‹¤í–‰ ê²°ê³¼ì˜ ë…¸ë“œ ê°ì²´

    Returns:
        Dict[str, Any]: agent_name, response_text, parsed_data, execution_time, token_usageë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
    """
    node_id = node.node_id
    text = "(ì‘ë‹µ ì—†ìŒ)"
    execution_time = 0.0
    usage = {}

    if hasattr(node, "result") and node.result:
        agent_result = node.result.result
        if hasattr(agent_result, "message") and agent_result.message:
            content = agent_result.message.get("content", [])
            if content and len(content) > 0:
                text = content[0].get("text", "")

        execution_time = node.result.execution_time / 1000  # ms -> s ë³€í™˜
        usage = getattr(node.result, "accumulated_usage", {})

    # JSON íŒŒì‹± ì¶”ê°€
    parsed_data = parse_agent_response(text, node_id)

    return {
        "agent_name": node_id,
        "response_text": text,
        "parsed_data": parsed_data,
        "execution_time": execution_time,
        "token_usage": usage,
    }


def run_multi_agent_evaluation(question: str, answer: str) -> Dict[str, Any]:
    """ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ ì‹¤í–‰í•˜ì—¬ í‰ê°€ ê²°ê³¼ë¥¼ ë°˜í™˜

    ë©˜í‹°ì˜ ì§ˆë¬¸ê³¼ ë©˜í† ì˜ ë‹µë³€ì„ ì…ë ¥ë°›ì•„ 4ê°œ ì—ì´ì „íŠ¸ë¡œ êµ¬ì„±ëœ
    í‰ê°€ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

    Args:
        question: ë©˜í‹°ì˜ ì§ˆë¬¸
        answer: ë©˜í† ì˜ ë‹µë³€

    Returns:
        Dict[str, Any]: ê° ì—ì´ì „íŠ¸ì˜ ì‘ë‹µ, ìµœì¢… í•©ì˜, ì‹¤í–‰ ì •ë³´ ë“±ì„ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
    """

    # ì…ë ¥ í¬ë§·íŒ… (ë©˜í‹° ì§ˆë¬¸ê³¼ ë©˜í†  ë‹µë³€ì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜)
    evaluation_input = f"""[ë©˜í‹° ì§ˆë¬¸]
{question}

[ë©˜í†  ë‹µë³€]
{answer}"""

    # ê·¸ë˜í”„ ì‹¤í–‰ (ì—ì´ì „íŠ¸ë“¤ì´ ì •ì˜ëœ ìˆœì„œëŒ€ë¡œ í‰ê°€ ìˆ˜í–‰)
    result = evaluation_graph(evaluation_input)

    # ì—ì´ì „íŠ¸ ì‘ë‹µ ì¶”ì¶œ (í—¬í¼ í•¨ìˆ˜ ì‚¬ìš©)
    agent_responses = [_extract_agent_response(node) for node in result.execution_order]

    # ìµœì¢… í•©ì˜ ê²°ê³¼ (ë§ˆì§€ë§‰ ì—ì´ì „íŠ¸ì¸ quality_consensusì˜ ì‘ë‹µ)
    final_consensus = (
        agent_responses[-1]["response_text"] if agent_responses else "(í‰ê°€ ì‹¤íŒ¨)"
    )

    # ì´ ì‹¤í–‰ ì‹œê°„ ë° í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
    total_execution_time = sum(resp["execution_time"] for resp in agent_responses)
    total_tokens = sum(
        resp["token_usage"].get("totalTokens", 0) for resp in agent_responses
    )

    return {
        "agent_responses": agent_responses,
        "final_consensus": final_consensus,
        "total_execution_time": total_execution_time,
        "total_tokens": total_tokens,
        "execution_order": [node.node_id for node in result.execution_order],
        "status": result.status,
    }


def calculate_grade(score: float, agent_data: Optional[Dict[str, Any]] = None) -> str:
    """DeepEval ì ìˆ˜ (0-10)ë¥¼ D/C/B/A/S ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜

    quality_consensus ì—ì´ì „íŠ¸ì˜ ê³¼ë½ ê·œì¹™ ì ìš©:
    - [ì‹¤í–‰ê°€ëŠ¥ì„±] ë˜ëŠ” [ì „ë¬¸ì„±]ì´ 1ì  ì´í•˜ì¼ ê²½ìš° ìµœëŒ€ Cë“±ê¸‰ìœ¼ë¡œ ì œí•œ

    Args:
        score: DeepEvalì—ì„œ ë°˜í™˜ëœ ì ìˆ˜ (0-10 ë²”ìœ„)
        agent_data: quality_consensusì˜ íŒŒì‹±ëœ ë°ì´í„° (optional)

    Returns:
        str: D, C, B, A, S ì¤‘ í•˜ë‚˜ì˜ ë“±ê¸‰
    """
    # DeepEvalì˜ scoreëŠ” 0-10 ë²”ìœ„
    absolute_score = score

    # ê¸°ë³¸ ë“±ê¸‰ ì‚°ì • (10ì  ë§Œì  ì²´ê³„)
    # D: 0-2, C: 3-4, B: 5-6, A: 7-8, S: 9-10
    if absolute_score >= 9:
        base_grade = "S"
    elif absolute_score >= 7:
        base_grade = "A"
    elif absolute_score >= 5:
        base_grade = "B"
    elif absolute_score >= 3:
        base_grade = "C"
    else:
        base_grade = "D"

    # ê³¼ë½ ê·œì¹™ ì ìš© (quality_consensus ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
    if agent_data and "final_evaluation" in agent_data:
        breakdown = agent_data["final_evaluation"].get("breakdown", {})
        actionability = breakdown.get("actionability", 4)
        expertise = breakdown.get("expertise", 4)

        # ì‹¤í–‰ê°€ëŠ¥ì„± ë˜ëŠ” ì „ë¬¸ì„±ì´ 1ì  ì´í•˜ë©´ Cë“±ê¸‰ìœ¼ë¡œ ì œí•œ
        if actionability <= 1 or expertise <= 1:
            if base_grade in ["S", "A", "B"]:
                return "C"

    return base_grade


# ==================== Phase 2: ë¹„ë™ê¸° í‰ê°€ í•¨ìˆ˜ ====================


async def run_multi_agent_evaluation_async(
    question: str, answer: str
) -> Dict[str, Any]:
    """ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰ (Phase 2)

    Strands ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì€ ë™ê¸° ë°©ì‹ì´ë¯€ë¡œ
    ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸°ë¡œ ë˜í•‘í•©ë‹ˆë‹¤.

    Args:
        question: ë©˜í‹°ì˜ ì§ˆë¬¸
        answer: ë©˜í† ì˜ ë‹µë³€

    Returns:
        Dict[str, Any]: ê° ì—ì´ì „íŠ¸ì˜ ì‘ë‹µ, ìµœì¢… í•©ì˜, ì‹¤í–‰ ì •ë³´ ë“±ì„ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
    """
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        _agent_executor,
        run_multi_agent_evaluation,
        question,
        answer,
    )


async def run_rubric_evaluation_async(
    question: str, answer: str, agent_consensus_data: Dict[str, Any]
) -> Dict[str, Any]:
    """DeepEvalì˜ Rubric ê¸°ë°˜ í‰ê°€ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰ (Phase 2)

    DeepEvalì˜ GEval ë©”íŠ¸ë¦­ì˜ ë¹„ë™ê¸° ë©”ì„œë“œ(a_measure)ë¥¼ ì‚¬ìš©í•˜ê³ ,
    ë²ˆì—­ë„ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    Args:
        question: ë©˜í‹°ì˜ ì§ˆë¬¸
        answer: ë©˜í† ì˜ ë‹µë³€
        agent_consensus_data: quality_consensus ì—ì´ì „íŠ¸ì˜ íŒŒì‹±ëœ ë°ì´í„° (JSON êµ¬ì¡°)

    Returns:
        Dict[str, Any]: ì ìˆ˜, í•©ê²© ì—¬ë¶€, í‰ê°€ ì´ìœ , ë¹„ìš© ë“±ì„ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
    """
    # quality_consensus ë°ì´í„°ë¥¼ ëª…í™•í•œ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
    final_eval = agent_consensus_data.get("final_evaluation", {})
    breakdown = final_eval.get("breakdown", {})

    # GEvalì´ ì´í•´í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_text = f"""
ë©€í‹° ì—ì´ì „íŠ¸ í‰ê°€ ê²°ê³¼:

[ìµœì¢… ë“±ê¸‰ ë° ì´ì ]
- ë“±ê¸‰: {final_eval.get('grade', 'N/A')}
- ì´ì : {final_eval.get('total_score', 0)}/10ì 

[ì„¸ë¶€ ì ìˆ˜ breakdown]
- ì‹¤í–‰ê°€ëŠ¥ì„± (Actionability): {breakdown.get('actionability', 0)}/4ì 
- ì „ë¬¸ì„± (Expertise): {breakdown.get('expertise', 0)}/4ì 
- í˜„ì‹¤ì„± (Context Fit): {breakdown.get('context_fit', 0)}/2ì 

[í•„ìˆ˜ ì¡°ê±´ ì¶©ì¡± ì—¬ë¶€]
- Essential Condition Met: {agent_consensus_data.get('essential_condition_met', True)}

[ì¢…í•© í‰ê°€]
{agent_consensus_data.get('summary_feedback', '')}

[ê°œì„  ì œì•ˆ]
{agent_consensus_data.get('integrated_improvement', '')}
"""

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±
    test_case = LLMTestCase(
        input=question,
        actual_output=answer,
        context=[context_text.strip()],
    )

    # DeepEvalì˜ ë¹„ë™ê¸° ë©”ì„œë“œ ì‚¬ìš©
    await QUALITY_METRIC.a_measure(test_case)

    # ë©”íŠ¸ë¦­ ê²°ê³¼ ì¶”ì¶œ
    score = QUALITY_METRIC.score
    reason = QUALITY_METRIC.reason

    # ë²ˆì—­ë„ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬
    reason_kr = await translate_to_korean_async(reason)

    return {
        "score": score,
        "threshold": QUALITY_METRIC.threshold,
        "success": QUALITY_METRIC.is_successful(),
        "reason_en": reason,
        "reason_kr": reason_kr,
        "evaluation_cost": QUALITY_METRIC.evaluation_cost,
        "evaluation_model": QUALITY_METRIC.evaluation_model,
    }


# ==================== ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì²˜ë¦¬ í•¨ìˆ˜ (Phase 2 ì—…ë°ì´íŠ¸) ====================


async def process_single_test_case(
    test_case: "TestCaseRequest", index: int
) -> "TestResultResponse":
    """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬ (Phase 2 ìµœì í™” ë²„ì „)

    Phase 2 ìµœì í™”:
    - ë¹„ë™ê¸° í•¨ìˆ˜ ì‚¬ìš© (run_multi_agent_evaluation_async, run_rubric_evaluation_async)
    - ThreadPoolExecutorë¥¼ í†µí•œ ìµœì í™”ëœ ìŠ¤ë ˆë“œ ê´€ë¦¬
    - ë²ˆì—­ë„ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬

    Args:
        test_case: í‰ê°€í•  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        index: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¸ë±ìŠ¤

    Returns:
        TestResultResponse: í‰ê°€ ê²°ê³¼

    Raises:
        Exception: í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ
    """
    async with _evaluation_semaphore:
        # Step 1: ë©€í‹° ì—ì´ì „íŠ¸ í‰ê°€ (Phase 2 ë¹„ë™ê¸° í•¨ìˆ˜ ì‚¬ìš©)
        agent_evaluation = await run_multi_agent_evaluation_async(
            test_case.input,
            test_case.actual_output,
        )

        # Step 2: quality_consensus íŒŒì‹±ëœ ë°ì´í„° ì¶”ì¶œ
        quality_consensus_data = None
        for agent_resp in agent_evaluation["agent_responses"]:
            if agent_resp["agent_name"] == "quality_consensus":
                quality_consensus_data = agent_resp.get("parsed_data")
                break

        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
        if not quality_consensus_data:
            quality_consensus_data = {
                "final_evaluation": {
                    "grade": "C",
                    "total_score": 5,
                    "breakdown": {"actionability": 2, "expertise": 2, "context_fit": 1},
                },
                "essential_condition_met": False,
                "summary_feedback": "JSON íŒŒì‹± ì‹¤íŒ¨",
                "integrated_improvement": "ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜ë¡œ í‰ê°€ ë¶ˆê°€",
            }

        # Step 3: Rubric í‰ê°€ (íŒŒì‹±ëœ ë°ì´í„° ì „ë‹¬)
        rubric_evaluation = await run_rubric_evaluation_async(
            test_case.input,
            test_case.actual_output,
            quality_consensus_data,
        )

        # Step 4: ë“±ê¸‰ ì‚°ì • (quality_consensus ë°ì´í„°ë¥¼ í™œìš©í•œ ê³¼ë½ ê·œì¹™ ì ìš©)
        # DeepEvalì˜ scoreëŠ” 0-1 ë²”ìœ„ì´ë¯€ë¡œ 10ì„ ê³±í•´ì„œ 0-10 ë²”ìœ„ë¡œ ë³€í™˜
        normalized_score = rubric_evaluation["score"]  # 0-1 ë²”ìœ„
        absolute_score = normalized_score * 10  # 0-10 ë²”ìœ„ë¡œ ë³€í™˜

        grade = calculate_grade(absolute_score, quality_consensus_data)

        # Step 5: ì‘ë‹µ êµ¬ì„±
        test_result = TestResultResponse(
            test_case_index=index,
            input=test_case.input,
            actual_output=test_case.actual_output,
            expected_output=test_case.expected_output,
            # ê° ì—ì´ì „íŠ¸ì˜ ì‘ë‹µì„ Pydantic ëª¨ë¸ë¡œ ë³€í™˜
            agent_responses=[
                AgentResponseDetail(**agent_resp)
                for agent_resp in agent_evaluation["agent_responses"]
            ],
            final_consensus=agent_evaluation["final_consensus"],
            # Rubric í‰ê°€ ê²°ê³¼ êµ¬ì„±
            rubric_evaluation=RubricEvaluationDetail(
                score=normalized_score,
                absolute_score=absolute_score,
                grade=grade,
                threshold=rubric_evaluation["threshold"],
                success=rubric_evaluation["success"],
                reason=rubric_evaluation["reason_kr"],
                reason_en=rubric_evaluation["reason_en"],
                evaluation_cost=rubric_evaluation["evaluation_cost"],
                evaluation_model=rubric_evaluation["evaluation_model"],
            ),
            # ì‹¤í–‰ ì •ë³´
            total_execution_time=agent_evaluation["total_execution_time"],
            total_tokens=agent_evaluation["total_tokens"],
            execution_order=agent_evaluation["execution_order"],
            success=rubric_evaluation["success"],
        )

        return test_result


# ==================== Pydantic ëª¨ë¸ ì •ì˜ ====================
# FastAPIì˜ ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆë¥¼ ì •ì˜í•˜ëŠ” ëª¨ë¸ë“¤


class TestCaseRequest(BaseModel):
    """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìš”ì²­ ëª¨ë¸"""

    input: str  # ë©˜í‹° ì§ˆë¬¸
    actual_output: str  # ë©˜í†  ë‹µë³€
    expected_output: Optional[str] = None  # ê¸°ëŒ€ ë‹µë³€ (ì„ íƒì‚¬í•­, í˜„ì¬ ë¯¸ì‚¬ìš©)


class EvaluationRequest(BaseModel):
    """í‰ê°€ ìš”ì²­ ëª¨ë¸ (ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ í¬í•¨)"""

    test_cases: List[TestCaseRequest]


class AgentResponseDetail(BaseModel):
    """ê°œë³„ ì—ì´ì „íŠ¸ì˜ ì‘ë‹µ ìƒì„¸ ì •ë³´"""

    agent_name: str  # ì—ì´ì „íŠ¸ ì´ë¦„
    response_text: str  # ì—ì´ì „íŠ¸ ì‘ë‹µ í…ìŠ¤íŠ¸
    parsed_data: Dict[str, Any]  # íŒŒì‹±ëœ JSON ë°ì´í„° (category, score, reasoning ë“±)
    execution_time: float  # ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
    token_usage: Dict[
        str, int
    ]  # í† í° ì‚¬ìš©ëŸ‰ (totalTokens, inputTokens, outputTokens ë“±)


class RubricEvaluationDetail(BaseModel):
    """Rubric ê¸°ë°˜ í‰ê°€ ìƒì„¸ ì •ë³´"""

    score: float  # ì •ê·œí™” ì ìˆ˜ (0-1 ë²”ìœ„, DeepEval ì›ë³¸ ì ìˆ˜)
    absolute_score: float  # ì ˆëŒ€ ì ìˆ˜ (0-10 ë²”ìœ„, score Ã— 10)
    grade: str  # D, C, B, A, S ë“±ê¸‰
    threshold: float  # í•©ê²© ê¸°ì¤€ì  (0-1 ë²”ìœ„, 0.5 = 5ì /10ì )
    success: bool  # í•©ê²© ì—¬ë¶€
    reason: str  # í‰ê°€ ì´ìœ  (í•œê¸€)
    reason_en: str  # í‰ê°€ ì´ìœ  (ì˜ë¬¸ ì›ë³¸)
    evaluation_cost: float  # í‰ê°€ ë¹„ìš©
    evaluation_model: str  # í‰ê°€ì— ì‚¬ìš©ëœ ëª¨ë¸ ì´ë¦„


class TestResultResponse(BaseModel):
    """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ í‰ê°€ ê²°ê³¼"""

    test_case_index: int  # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¸ë±ìŠ¤
    input: str  # ë©˜í‹° ì§ˆë¬¸
    actual_output: str  # ë©˜í†  ë‹µë³€
    expected_output: Optional[str]  # ê¸°ëŒ€ ë‹µë³€ (ì„ íƒì‚¬í•­)

    # ë©€í‹° ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼
    agent_responses: List[AgentResponseDetail]  # ê° ì—ì´ì „íŠ¸ì˜ ì‘ë‹µ
    final_consensus: str  # quality_consensusì˜ ìµœì¢… ë¦¬í¬íŠ¸

    # Rubric ê¸°ë°˜ í‰ê°€ ê²°ê³¼
    rubric_evaluation: RubricEvaluationDetail

    # ì‹¤í–‰ ì •ë³´
    total_execution_time: float  # ì´ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
    total_tokens: int  # ì´ í† í° ì‚¬ìš©ëŸ‰
    execution_order: List[str]  # ì—ì´ì „íŠ¸ ì‹¤í–‰ ìˆœì„œ

    # ì „ì²´ ì„±ê³µ ì—¬ë¶€
    success: bool  # Rubric í‰ê°€ í•©ê²© ì—¬ë¶€


class EvaluationResponse(BaseModel):
    """ì „ì²´ í‰ê°€ ì‘ë‹µ (ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ í¬í•¨)"""

    test_results: List[TestResultResponse]


# ==================== FastAPI ì—”ë“œí¬ì¸íŠ¸ ====================


@app.post("/evaluate", response_model=EvaluationResponse)
async def evaluate_test_cases(request: EvaluationRequest):
    """ë©˜í† ë§ ë‹µë³€ì„ í‰ê°€í•˜ëŠ” ë©”ì¸ API ì—”ë“œí¬ì¸íŠ¸ (Phase 2: ë¹„ë™ê¸° ìµœì í™” ë²„ì „)

    ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ë‘ ë‹¨ê³„ì˜ í‰ê°€ í”„ë¡œì„¸ìŠ¤ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    1. ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ í†µí•œ ì •ì„±ì  ë¶„ì„
       - action_master: ì‹¤í–‰ ì§€ì¹¨ êµ¬ì²´ì„± í‰ê°€
       - pro_proof: ì‹¤ë¬´ ì „ë¬¸ì„± ê²€ì¦
       - context_guardian: í˜„ì‹¤ì„± ë¶„ì„
       - quality_consensus: ì¢…í•© ë¦¬í¬íŠ¸ ì‘ì„±

    2. DeepEval Rubric ê¸°ë°˜ ì •ëŸ‰ì  ì ìˆ˜ ì‚°ì¶œ
       - ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ 0-10 ìŠ¤ì¼€ì¼ ì ìˆ˜ ì‚°ì¶œ
       - D/C/B/A/S ë“±ê¸‰ ìë™ ì‚°ì •

    Phase 1 ìµœì í™”:
    - ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ 60-80% ì„±ëŠ¥ í–¥ìƒ
    - Semaphoreë¡œ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ (ê¸°ë³¸ 5ê°œ)
    - ì¼ë¶€ ì‹¤íŒ¨ ì‹œì—ë„ ë¶€ë¶„ ê²°ê³¼ ë°˜í™˜

    Phase 2 ìµœì í™”:
    - ë¹„ë™ê¸° í•¨ìˆ˜ ì‚¬ìš© (run_multi_agent_evaluation_async, run_rubric_evaluation_async)
    - ThreadPoolExecutorë¥¼ í†µí•œ ìµœì í™”ëœ ìŠ¤ë ˆë“œ ê´€ë¦¬
    - ë²ˆì—­ë„ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬ (translate_to_korean_async)
    - DeepEvalì˜ a_measure() ë¹„ë™ê¸° ë©”ì„œë“œ í™œìš©

    Args:
        request: í‰ê°€í•  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤ì„ í¬í•¨í•œ ìš”ì²­ ê°ì²´

    Returns:
        EvaluationResponse: ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ í‰ê°€ ê²°ê³¼ë¥¼ í¬í•¨í•œ ì‘ë‹µ ê°ì²´
            - ê° ì—ì´ì „íŠ¸ì˜ ìƒì„¸ ë¶„ì„
            - ìµœì¢… í•©ì˜ ë¦¬í¬íŠ¸
            - Rubric ì ìˆ˜ ë° ë“±ê¸‰
            - ì‹¤í–‰ ì‹œê°„ ë° í† í° ì‚¬ìš©ëŸ‰

    Example:
        Request:
        {
            "test_cases": [
                {
                    "input": "ì£¼ë‹ˆì–´ ê°œë°œìì¸ë° ì½”ë“œ ë¦¬ë·°ë¥¼ ì˜ ë°›ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
                    "actual_output": "ì½”ë“œ ë¦¬ë·°ë¥¼ ì˜ ë°›ìœ¼ë ¤ë©´..."
                }
            ]
        }

        Response:
        {
            "test_results": [
                {
                    "test_case_index": 0,
                    "rubric_evaluation": {
                        "score": 0.85,
                        "grade": "A",
                        ...
                    },
                    ...
                }
            ]
        }
    """

    # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ë³‘ë ¬ ì²˜ë¦¬
    tasks = [process_single_test_case(tc, i) for i, tc in enumerate(request.test_cases)]

    # ë³‘ë ¬ ì‹¤í–‰ (ì¼ë¶€ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # ì—ëŸ¬ ì²˜ë¦¬ (ì¼ë¶€ ì‹¤íŒ¨í•´ë„ ë¶€ë¶„ ê²°ê³¼ ë°˜í™˜)
    processed_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            # ì—ëŸ¬ ë°œìƒ ì‹œ ì—ëŸ¬ ì‘ë‹µ ìƒì„±
            error_result = TestResultResponse(
                test_case_index=i,
                input=request.test_cases[i].input,
                actual_output=request.test_cases[i].actual_output,
                expected_output=request.test_cases[i].expected_output,
                agent_responses=[],
                final_consensus=f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(result)}",
                rubric_evaluation=RubricEvaluationDetail(
                    score=0.0,
                    absolute_score=0.0,
                    grade="D",
                    threshold=0.7,
                    success=False,
                    reason=f"í‰ê°€ ì‹¤íŒ¨: {str(result)}",
                    reason_en=f"Evaluation failed: {str(result)}",
                    evaluation_cost=0.0,
                    evaluation_model="N/A",
                ),
                total_execution_time=0.0,
                total_tokens=0,
                execution_order=[],
                success=False,
            )
            processed_results.append(error_result)
        else:
            processed_results.append(result)

    return {"test_results": processed_results}


@app.get("/")
def root():
    """API ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸

    API ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì¸ì§€ í™•ì¸í•˜ëŠ” í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤.

    Returns:
        dict: API ìƒíƒœ ë©”ì‹œì§€
    """
    return {"message": "CoEval API is running"}
